<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[我的音视频技术路线]]></title>
    <url>%2F2020%2F03%2F30%2F%EF%BC%88url%E4%B8%AD%E6%98%BE%E7%A4%BA%E7%9A%84%E6%A0%87%E9%A2%98%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 抖音/快手等短视频APP的风靡，让音视频成为当下最火热的技术，越来越多的人想要进入到这个领域，我自己也是从图形方向刚刚踏入这领域不久，音视频方向所包含的技术栈非常复杂，我自己也在一点一点慢慢钻研，这里面每一个方向都值得深入研究，而且随着5G时代的到来，音视频方向的应用会更加广泛，所以希望自己能掌握更多的关于音视频方向的技能，未来可以探索更多的音视频玩法。然后这篇博客主要是想梳理一下我自己关于音视频这个方向的学习路线，分享出来的同时也能鼓励自己朝着这个方向继续深耕下去。 ​ 关于音视频方向的基础技能分支，先来看一张图（图片来自网上） 采集：音视频数据来源，比如Android Camera数据采集 渲染：将采集得到的数据展示到Surface上，并添加一些图形效果 处理：对源数据的加工，比如添加滤镜、特效，还有多视频剪辑、变速、转场等等 编解码：对音视频数据进行压缩封装，减少数据量，方便传输 传输：对采集加工完成的数据传输至客户端，比如直播推流、拉流 1. 关于音视频数据采集(Android)​ 因为自己主要是对Android Camera比较熟悉，所以我主要梳理一下这方面的知识点，我会从最基础的Android Camera API的接口以及基本流程开始梳理，后面进阶部分主要是结合Camera HAL高通架构进一步详细梳理底层的Camera原理，最后是我对于Camera专业视频方向的一些探索。 1.1 Android Camera API​ Android 5.0之后Camera接口升级成API2，因为Camera API 1接口过于简单，根本体现不出硬件能力，用户能控制的不多，比如拿不到RAW数据，控制不了相机参数的下发等等，如下代码看下他内部的基本接口 ​ Camera API 1 1234567891011121314151617181920212223242526272829303132try &#123; mCamera = Camera.open(mCurrentCamera); Camera.Parameters params = mCamera.getParameters(); List&lt;Camera.Size&gt; previewSizes = params.getSupportedPreviewSizes(); Camera.Size preViewSize = previewSizes.get(previewSizes.size() &gt; 4 ? previewSizes.size() - 4 : 0); params.setPreviewSize(preViewSize.width, preViewSize.height); mPreviewHeight = preViewSize.height; mPreviewWidth = preViewSize.width; Log.e(TAG, "preViewSize-&gt;width: " + preViewSize.width + ", preViewSize-&gt;height: " + preViewSize.height); params.setPictureFormat(ImageFormat.JPEG); params.setJpegQuality(100); //是否开启闪光 List&lt;String&gt; flashModes = params.getSupportedFlashModes(); if (flashModes != null &amp;&amp; flashModes.contains(Camera.Parameters.FLASH_MODE_OFF)) &#123; params.setFlashMode(Camera.Parameters.FLASH_MODE_OFF); &#125; mCamera.setPreviewCallback(this); mCamera.setDisplayOrientation(PORTRAIT_MODE); mCamera.setParameters(params); if(!Consts.SHOW_CAMERA) &#123; LogUtils.log_main_step("相机开始preview"); mCamera.startPreview(); &#125; LogUtils.logd(TAG, "camera init finish....."); &#125; catch (Exception e) &#123; LogUtils.loge(TAG, e.getMessage()); e.printStackTrace(); &#125; 然后在CameraPreview callback里面就可以处理相机数据了 12345678910111213141516171819202122@Override public void onPreviewFrame(byte[] data, Camera camera) &#123; Camera.CameraInfo mCameraInfo = new Camera.CameraInfo(); //如果使用前置摄像头，请注意显示的图像与帧图像左右对称，需处理坐标 boolean frontCamera = (mCurrentCamera == Camera.CameraInfo.CAMERA_FACING_FRONT); //获取重力传感器返回的方向 int dir = getDirection(); int rotate = (dir ^ 1); //Log.e("stRotateCamera ", (dir ^ 1) + " rotate result"); //在使用后置摄像头，且传感器方向为0或2时，后置摄像头与前置orentation相反 if (!frontCamera &amp;&amp; dir == 0) &#123; dir = 2; &#125; else if (!frontCamera &amp;&amp; dir == 2) &#123; dir = 0; &#125; dir = (dir ^ 2); //..... process(data) &#125; 关于Camera2的介绍 Camera 2.0: New computing platforms for computational photography ​ Camera API2对比API 1改动非常大，主要配合HAL3进行使用，功能和接口都更加齐全，同时使用起来也会更加复杂，但如果熟悉之后，也能拍摄出更加丰富的效果。下图关于Camera API 2的几个使用场景 ​ 然后结合具体代码讲几个Camera2的主要接口 1.CameraManager 关于硬件能力的统一封装接口 12345678CameraManager cameraManager = (CameraManager)mContext.getSystemService(Context.CAMERA_SERVICE);//可以拿到所以的相机列表，比如Wide，Tele，Macro，Tele2x，Tele4x等等String[] cameraIdList = cameraManager.getCameraIdList();//根据Camera ID拿到对应设备支持的能力CameraCharacteristics cameraCharacteristics = cameraManager.getCameraCharacteristics(cameraIdStr);//比如用这个去判断支持的最小Focus distanceFloat focusDistance = mCharacteristics.get(CameraCharacteristics.LENS_INFO_MINIMUM_FOCUS_DISTANCE); ​ 现如今机型的摄像头的数量越来越多，组合也越来越丰富，不同的Camera负责不同的能力，比如我们需要更大的拍摄范围会选择Ultra Wide，比如小米一亿像素的Wide，还有Macro等等。 2.CaptureDevice 我们可以在OpenCamera Callback拿到CameraDevice 1234567891011121314151617181920212223242526272829String cameraIdStr = String.valueOf(mCameraId); cameraManager.openCamera(cameraIdStr, mCameraStateCallback, mMainHandler);//private CameraDevice.StateCallback mCameraStateCallback = new CameraDevice.StateCallback() &#123; @Override public void onOpened(@NonNull CameraDevice camera) &#123; synchronized (SnapCamera.this) &#123; mCameraDevice = camera; &#125; if (mStatusListener != null) &#123; mStatusListener.onCameraOpened(); &#125; &#125; @Override public void onDisconnected(@NonNull CameraDevice camera) &#123; Log.w(TAG, "onDisconnected"); // fail-safe: make sure resources get released release(); &#125; @Override public void onError(@NonNull CameraDevice camera, int error) &#123; Log.e(TAG, "onError: " + error); // fail-safe: make sure resources get released release(); &#125; &#125;; 3.CaptureRequest ​ 通过CameraDevice可以创建CaptureRequest，类型主要有以下几种，1-6分布用于预览、拍照、录制、录制中拍照、ZSL、手动。 123456789101112public static final int TEMPLATE_MANUAL = 6;public static final int TEMPLATE_PREVIEW = 1; public static final int TEMPLATE_RECORD = 3;public static final int TEMPLATE_STILL_CAPTURE = 2;public static final int TEMPLATE_VIDEO_SNAPSHOT = 4;public static final int TEMPLATE_ZERO_SHUTTER_LAG = 5;//创建的代码mPreviewRequestBuilder = mCameraDevice.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);//这里是更加专业一点的用法，可以不看，就是拿到RequestBuilder之后我们可以去下发各种TAG，如下设置AE/AWB锁CaptureRequestBuilder.applyAELock(request, mConfigs.isAELocked());CaptureRequestBuilder.applyAWBLock(request, mConfigs.isAWBLocked()); 4.Surface Surface主要是用来接相机返回的数据，可以支持不同的数据类型和分辨率 1234567891011//SurfaceTexture的方式，用于预览mSurfaceTexture = new SurfaceTexture(false);mSurfaceTexture.setDefaultBufferSize(optimalSize.width, optimalSize.height);mPreviewSurface = new Surface(mSurfaceTexture);//ImageReader 的方式, Format可以是YUV、RAW，DEPTH等mPhotoImageReader = ImageReader.newInstance(size.getWidth(), size.getHeight(), ImageFormat.JPEG, /* maxImages */ 2);mPhotoImageReader.setOnImageAvailableListener(mPhotoAvailableListener, mCameraHandler);//创建sessionList&lt;Surface&gt; surfaces = Arrays.asList(mPreviewSurface, mPhotoImageReader.getSurface());mCameraDevice.createCaptureSession(surfaces, mSessionCallback, mCameraHandler); 5.CaptureSession 通过上面的CameraDevice创建Session，在Callback里面拿到Session 1234567891011121314151617181920212223mCameraDevice.createCaptureSession(surfaces, mSessionCallback, mCameraHandler);private CameraCaptureSession.StateCallback mSessionCallback = new CameraCaptureSession.StateCallback() &#123; @Override public void onConfigured(@NonNull CameraCaptureSession session) &#123; synchronized (SnapCamera.this) &#123; if (mCameraDevice == null) &#123; Log.e(TAG, "onConfigured: CameraDevice was already closed."); session.close(); return; &#125; mCaptureSession = session; &#125; startPreview(); capture(); &#125; @Override public void onConfigureFailed(@NonNull CameraCaptureSession session) &#123; Log.e(TAG, "sessionCb: onConfigureFailed"); &#125; &#125;; ​ 后面所有的采集工作都是基于Session，关于Session机制最早是诺基亚提出来的，后面苹果的IOS也采取Session作为相机拍照，录制的会话单元。 123456789/*发起请求，后续相机开始往surface输出数据，这个可以在onConfigured里面调用，如上面的代码 startPreview();capture();*/mCaptureSession.setRepeatingRequest(mPreviewRequestBuilder.build(), mCaptureCallback, mCameraHandler);mCaptureSession.capture(requestBuilder.build(), mCaptureCallback, mCameraHandler);// 这个是针对120/240/960Fps recordingmCaptureSession.setRepeatingBurst(requestList, mCaptureCallback, mCameraHandler);mCaptureSession.captureBurst(requestList, listener, handler); 然后还可以通过Session控制结束动作 12mCaptureSession.abortCaptures();mCaptureSession.stopRepeating(); ​ 对象更加专业一点的系统相机而言，在setRepeatingRequest之前可以还需要做很多工作，比如拍照要等Focus finish，还需要Apply一些列参数，比如FocusMode, Zoom, AE/F Lock, Video FPS….. 总结：上面只是一个大概流程，至于每个流程你需要去控制什么可能就会更加复杂，但其实对于第三方APP，需要控制的不多，基本就是OpenCamera -&gt; 配置Surface -&gt; 创建Session -&gt; 设置参数 -&gt; RepeatingRequest -&gt;ImageReader或者GLSurafce回调接数据做后续处理。可能有些更加专业一点的相机应用，可能会涉及的一些专业参数的下发，比如IOS, FocusDistance, Shutter Time, EV…. 1.2 Camera HAL Android Camera硬件抽象层（HAL，Hardware Abstraction Layer）主要用于把底层camera drive与硬件和位于android.hardware中的framework APIs连接起来。Camera子系统主要包含了camera pipeline components 的各种实现，而camera HAL提供了这些组件的使用接口 官网的系统架构图： ​ 做相机开发除了第一部分讲到的APP层面的相机控制，然后就是HAL层的开发了，HAL层由芯片厂商定制(比如高通等),手机厂商可以再其上增加一些内容。Camera HAL 经历1-3的版本迭代，不同的硬件支持的Camera2程度不一样，主要有以下等级 123456//每一个等级啥意思自己搜吧INFO_SUPPORTED_HARDWARE_LEVEL_LEGACYINFO_SUPPORTED_HARDWARE_LEVEL_LIMITEDINFO_SUPPORTED_HARDWARE_LEVEL_FULLINFO_SUPPORTED_HARDWARE_LEVEL_3INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL 想了一下这一块比较复杂，我自己也不是非常的清楚，而且好像第三方APP一般不会涉及，后面有机会单独整理吧(挖坑1) 1.3 CameraX​ 如上看到的Camera API2非常复杂，为了简化流程，Google推出CameraX，借助 CameraX，开发者只需两行代码就能利用与预安装的相机应用相同的相机体验和功能。 CameraX Extensions 是可选插件，通过该插件，您可以在支持的设备上向自己的应用中添加人像、HDR、夜间模式和美颜等效果。如下预览和拍照的代码： 123456789101112131415161718192021222324252627//预览PreviewConfig config = new PreviewConfig.Builder().build();Preview preview = new Preview(config); preview.setOnPreviewOutputUpdateListener( new Preview.OnPreviewOutputUpdateListener() &#123; @Override public void onUpdated(Preview.PreviewOutput previewOutput) &#123; // Your code here. For example, use previewOutput.getSurfaceTexture() // and post to a GL renderer. &#125;; &#125;); CameraX.bindToLifecycle((LifecycleOwner) this, preview);//拍照ImageCaptureConfig config =new ImageCaptureConfig.Builder() .setTargetRotation(getWindowManager().getDefaultDisplay().getRotation()) .build();ImagesCapture imageCapture = new ImageCapture(config);CameraX.bindToLifecycle((LifecycleOwner) this, imageCapture, imageAnalysis, preview);ImageCapture.Metadata metadata = new ImageCapture.Metadata(); metadata.isReversedHorizontal = mCameraLensFacing == LensFacing.FRONT;imageCapture .takePicture(saveLocation, metadata, executor, OnImageSavedListener); 1.4 Camera专业视频(进阶)​ 现如今手机相机功能越来越强大，现在已经基本可以取代卡片机，手机相机已经作为手机厂商最重要的一个卖点，尤其是DXO刷榜的行为，之后让厂商投入更多的研发相机上面，在可以预见的未来，手机相机发展势必会更加激进，后面进一步取代部分单反也是有可能的，所以相机未来应该更多的体现起作为生产力工具的一部分，尤其是现在视频作为人们记录生活的方式，越来越得到普及。 ​ 如果说到视频和相机，我的想法是如何去拍摄更加专业的视频，借助手机相机强大的功能，能否让一部分专业人士用他作为生产力工具，部分取代非常不便携的单反设备，比如借助Camera2参数调节功能去实现长曝光、延时、流光快门等效果。 ​ 相机更加专业的方向，主要体现手机厂商的系统相机，尤其是Android端，很多硬件能力只能由手机厂商自己控制，比如系统相机里面的专业模式，还有就是类似大疆这种，也是自己做相机硬件，然后能够结合硬件能力，去实现一些更加专业的拍摄效果。目前软件这块做的比较专业的就是Fimic Pro。 ​ ​ 支持各种调参：ISO ，Exposure time，WB，EV，Focus distance ​ 支持平滑变焦，光焦分离 ​ 支持曲线调整：亮度、饱和度、阴影、Gamma曲线等 ​ 支持LOG格式，还有专业的音频采集 ​ 有非常多的辅助信息，比如峰值对焦、曝光反馈、RGB直方图信息等 ​ 音视频各种格式自定义：画幅比例、FPS、分辨率、音频采样率、编码格式等等 ​ 真的是一个非常强大的生产力工具 2. 关于图形渲染​ 音视频第二部分肯定是图形渲染方向啦，因为之前一直有做图形渲染方面的工作，也写过自己的渲染引擎，链接如下 ​ https://github.com/LukiYLS/SimpleRenderer 以及关于这个引擎的介绍 ​ http://yanglusheng.com/ ​ 所以可以大概分享一下自己的学习过程，以及关于Android GLES相关总结 2.1 图形学基础​ 我觉图形方面基础的应该需要掌握如下： 1.整个图形矩阵变换过程 World Matrix：将场景中所有对象统一到一个坐标系下 ViewMatrix：World Matrix 变换的相机坐标系，根据相机的三个参数生成 ProjectionMatrix：3D世界投影的2D平面 NDC：转换到[-1 1] Screen Matrix：转换的屏幕坐标系 这部分想要理解就要自己用笔手推一遍，非常管用 2.OpenGL API 熟悉API，比如纹理贴图方式，绘制点线面，VAO/VBO创建等等 3.光照 首先需要理解传统的Phong光照，然后要看PBR，理解BRDF模型和公式推导 还有就是Shadow这快，理解shadowmap的原理，不同的光照怎样生成深度图，然后shadow acne，处理边缘锯齿等很多细节，还有阴影体这块 4.模版测试/深度测试 深度测试实现遮挡 模版测试也是非常有用，比如我有篇博客里面讲到的 阴影体结合模版测试实现矢量紧贴地形的效果 5.地形 怎用利用perlin noise生成地形顶点数据 LOD的地形：规则四叉树划分(Google Earth地形)，以及不规则的CLOD(自适应三角网) 6.粒子系统 主要就是怎么控制粒子发射器，粒子加速的，粒子运动轨迹等等 7.场景管理 如何利用四叉树、八叉树管理场景节点，做视锥体裁剪 …….. 之前写的渲染引擎，都包含这些基本模块，大概持续完善了半年左右，对我图形渲染方面的提升非常大，包括自己也写过软光栅器。 总结： 学习图形学最好的方式就是造轮子造轮子造轮子，自己写引擎，自己写软光栅器 学习资料分享： https://learnopengl-cn.github.io/ https://www.scratchapixel.com/ 阅读源码：OGRE/OSG/THREEJS 工具: unity3d processing 2.2 OpenGL/GLES1.EGL环境创建 eglGetDisplay //获取display信息 eglChooseConfig //设置RGBA bit depth eglCreatePbufferSurface // 创建离屏surface, 也可以eglCreateWindowSurface eglCreateContext //创建上下文 2.GL多线程 SharedContext多线程 这个网上也有很多资料，eglCreateContext 的时候传入其它线程的Context，既可以共享一些GPU Buffer，比如：MediaCodec录制的用的Surface，和预览去sharedContext Render pass如何做同步glFenceSync 如果用glFinish可能会在某个点等的时间很长，可以用glFenceSync做同步，非常不错 3.渲染优化 ​ 帧率如何优化，涉及到很多方面，这方面游戏引擎有很多技巧，比如提前视锥体裁剪，遮挡剔除等等，我这里只是简单讲一下在不涉及到大场景的优化有哪些 个人关于效率优化的总结： ​ 尽量在渲染之前先做视锥体裁剪工作，减少不必要的IO ​ 尽量少使用一些同步阻塞操作，比如glReadPixel、glFinish、glTeximage2D等 ​ Shader里面少使用if/for这种操作 ​ 必要时做下采样 ​ 利用好内存对齐，会有意想不到的效率提升 ​ 渲染之前做好一些列准备工作，比如编译Shader 2.3 音视频渲染引擎(扒抖音)​ 现在短视频应用关于特效部分底层都有会有一套渲染引擎，不管的抖音还是快手，你把抖音的的APP package pull出来就能看到，里面是同lua脚本去调用底层的渲染引擎，lua脚本负责下发一些参数，主要是AI的一些识别结果以及用户的交互事件。 ​ 虽然没有游戏引擎那么强大，但基本模块应该都会包含模型、资源管理、相机控制、裁剪、渲染等等，毕竟写这套引擎的基本都是以前搞游戏的那拨人，算是降维来写音视频渲染引擎。 ​ 下面通过拆解抖音内部的资源文件，探究内部关于音视频渲染引擎的模块组成，对比短视频引擎和游戏引擎的区别。 一起来看看抖音最近很火的一个游戏：潜水艇，通过移动鼻子控制潜水艇 如下是download 的资源包 ​ 这里面的核心控制逻辑就是那个lua基本，里面会负责把人脸关键点信息传给底层引擎，实时更新潜水艇的位置，脚本实现了两个碰撞检测函数，用于潜水艇和柱子之间做碰撞检测。Collision玩过游戏引擎的都很熟悉，有些游戏引擎Collision detect做的不好的会出现穿模的现象。 ​ rectCollision: 潜水艇中心坐标和柱子底部坐标之间的距离，与潜水艇半径比较，小于0.9*R，就是碰撞了 ​ circleCollision: 两个圆心坐标之间的距离和他们各自半径之和比较，小于半径和，就是碰撞了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364local intpx = 220 --柱子左右间隔local intpy = 550 --柱子中间缝隙宽度local range_y = &#123;0.3, 0.7&#125; --柱子缝隙中央随机范围，&#123;0.3,0.7&#125;代表缝隙可能在屏幕高度30%-70%的位置随机出现local sp = &#123;0.5,1.0&#125; --速度初始和最终速度，&#123;0.5,1.0&#125;代表初始速度为1秒走过0.5个屏幕，最终速度为1秒走过1个屏幕local ptime = 3 --准备时间3slocal range_s = &#123;5,10&#125; --分数分段，&#123;5,10&#125;代表0-5第一段，6-10第二段，10以上第三段；10以上未碰撞第四段local smul = 0.8 --字号倍率 //.....此次省略N行 //两个碰撞检测函数//u=1,2 代码上面和下面柱子local function rectCollision(i,u) local K_s_x = sub.x local K_s_y = sub.y * ratio local K_rect_x = pillar[i].x local K_rect_y = 0 local l = K_rect_x - r local r = K_rect_x + r local t = 0 local b = 0 if u == 1 then t = -1 b = (pillar[i].y - interval_y / 2) * ratio else t = (pillar[i].y + interval_y / 2) * ratio b = 2 end local closestP_x = 0.0 local closestP_y = 0.0 closestP_x = clamp(K_s_x, l , r) closestP_y = clamp(K_s_y, t , b) local dist = distance(closestP_x, closestP_y, K_s_x, K_s_y) if dist &lt;= sub.r * 0.9 then return true else return false end return falseendlocal function circleCollision(i,u) local K_s_x = sub.x local K_s_y = sub.y * ratio local K_cir_x = pillar[i].x local K_cir_y = 0 if u == 1 then K_cir_y = (pillar[i].y - interval_y / 2) * ratio else K_cir_y = (pillar[i].y + interval_y / 2) * ratio end local dist = distance(K_cir_x, K_cir_y, K_s_x, K_s_y) if dist &lt;= sub.r * 0.9 + r then return true else return false end return falseend 还有两个重要的函数，分别处理预览和录制，游戏过程的逻辑，比如柱子随着时间线一直在移动，speed在加快 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152handleTimerEvent = function(this, timerId, milliSeconds) if timerId == timer_ID_Fast and gaming then if init_state ~= 0 then return true end timeThis = getTime(this) timeDelta = getDiffTime(timeLast, timeThis) timeCumu = timeCumu + timeDelta timeLast = timeThis local speed = sp[1] + clamp(timeCumu / 14, 0,1) * (sp[2] - sp[1]) if timeCumu &gt; 14 then gaming = false Sticker2DV3.playClip(this, feature_0.folder, feature_0.entity[1], feature_0.clip[1], 0) CommonFunc.setFeatureEnabled(this, feature_t.folder, true) if score &gt;= 10 then realTimeFunc.initRealTime(this, feature_t.folder, true, feature_t.realTimeParams[1]) else realTimeFunc.initRealTime(this, feature_t.folder, true, feature_t.realTimeParams[2]) end realTimeFunc.setRealTime(this, feature_t.folder, "u_data", score) end for i = 1,5 do pillar[i].x = pillar[i].x - speed * timeDelta if pillar[i].x &lt; -r then pillar[i].x = pillar[i].x + interval_x * 5 pillar[i].y = range_y[1] + math.random() * (range_y[2] - range_y[1]) pillar[i].counted = false end if pillar[i].x &lt; sub.x and not pillar[i].counted then pillar[i].counted = true score = score + 1 end local cy = pillar[i].y - (interval_y / 2 - r / ratio) - 0.5 local cx = pillar[i].x set4Vtx(this, feature_3.folder, feature_3.entity[1], feature_3.clip[2*i-1], cx , cy, 2 * r, 1.0 , 0.0, 1.0, ratio, 0.0, 0.0) cy = pillar[i].y + (interval_y / 2 - r / ratio) + 0.5 //更新实体坐标 set4Vtx(this, feature_3.folder, feature_3.entity[1], feature_3.clip[2*i], cx , cy, 2 * r, 1.0 , 0.0, 1.0, ratio, 0.0, 0.0) end if noseY ~= 0 then sub.y = noseY end if (sub.y ~= 0 or lastY ~= 0) then sub.a = sub.a * 0.8 + (sub.y - lastY) / timeDelta * 0.2 else sub.a = sub.a * 0.8 end set4Vtx(this, feature_2.folder, feature_2.entity[1], feature_2.clip[1], sub.x , sub.y, 2 * sub.r, 2 * sub.r / ratio / 1.1 , sub.a, 1.0, ratio, 0.0, 0.0) //对所有柱子遍历做碰撞检测 for i = 1,5 do if rectCollision(i,1) or rectCollision(i,2) or circleCollision(i,1) or circleCollision(i,2) then gaming = false if score &lt;= range_s[1] then Sticker2DV3.playClip(this, feature_0.folder, feature_0.entity[1], feature_0.clip[4], 0) elseif score &lt;= range_s[2] then Sticker2DV3.playClip(this, feature_0.folder, feature_0.entity[1], feature_0.clip[3], 0) else Sticker2DV3.playClip(this, feature_0.folder, feature_0.entity[1], feature_0.clip[2], 0) end CommonFunc.setFeatureEnabled(this, feature_t.folder, true) if score &gt;= 10 then realTimeFunc.initRealTime(this, feature_t.folder, true, feature_t.realTimeParams[1]) else realTimeFunc.initRealTime(this, feature_t.folder, true, feature_t.realTimeParams[2]) end realTimeFunc.setRealTime(this, feature_t.folder, "u_data", score) end end lastY = sub.y end return true end, handleRecodeVedioEvent = function (this, eventCode) if (init_state ~= 0) then return true end if (eventCode == 1) then timeLast = getTime(this) timeCumu = 0 score = 0 gaming = true pillar = &#123; &#123; x = start_x, y = range_y[1] + math.random() * (range_y[2] - range_y[1]), counted = false &#125;, &#123; x = start_x + interval_x, y = range_y[1] + math.random() * (range_y[2] - range_y[1]), counted = false &#125;, &#123; x = start_x + interval_x * 2, y = range_y[1] + math.random() * (range_y[2] - range_y[1]), counted = false &#125;, &#123; x = start_x + interval_x * 3, y = range_y[1] + math.random() * (range_y[2] - range_y[1]), counted = false &#125;, &#123; x = start_x + interval_x * 4, y = range_y[1] + math.random() * (range_y[2] - range_y[1]), counted = false &#125;, &#125; sub = &#123; r = 0.105, x = 0.24, y = 0.5, sy = 0.0, a = 0 &#125; lastY = noseY for i = 1,10 do Sticker2DV3.playClip(this, feature_3.folder, feature_3.entity[1], feature_3.clip[i], 0) set4Vtx(this, feature_3.folder, feature_3.entity[1], feature_3.clip[i], -2 , -2, 0, 0 , 0.0, 0.0, ratio, 0.0, 0.0) end for i = 1,4 do Sticker2DV3.stopClip(this, feature_0.folder, feature_0.entity[1], feature_0.clip[i]) end set4Vtx(this, feature_2.folder, feature_2.entity[1], feature_2.clip[1], -2 , -2, 0, 0 , 0, 0, ratio, 0.0, 0.0) CommonFunc.setFeatureEnabled(this, feature_t.folder, false) end return true end, &#125; ​ 然后有几个stcker文件，可以看到有一个是柱子的entity信息，有一个是潜水艇的entity信息，后面那个游戏失败时弹出的那个框框模型。然后每一个stcker文件夹都有两个json文件，其中主要是clip.json，里面主要包含实体的基本信息，还有transform参数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970"clipname1": &#123; "alphaFactor": 1.0, "blendmode": 0, "fps": 16, "height": 801, "textureIdx": &#123; "idx": [ 0 ], "type": "image" &#125;, "transformParams": &#123; "position": &#123; "point0": &#123; "anchor": [ 0.0, 0.5 ], "point": [ &#123; "idx": "topright", "relationRef": 0, "relationType": "foreground", "weight": 0.5635420937542709 &#125;, &#123; "idx": "bottomleft", "relationRef": 0, "relationType": "foreground", "weight": -0.0793309886223863 &#125; ] &#125;, "point1": &#123; "anchor": [ 1.0, 0.5 ], "point": [ &#123; "idx": "topright", "relationRef": 0, "relationType": "foreground", "weight": 0.7854868849874516 &#125;, &#123; "idx": "bottomleft", "relationRef": 0, "relationType": "foreground", "weight": -0.0793309886223863 &#125; ] &#125; &#125;, "relation": &#123; "foreground": 1 &#125;, "relationIndex": [ 0 ], "relationRefOrder": 0, "rotationtype": 1, "scale": &#123; "scaleY": &#123; "factor": 1.0 &#125; &#125; &#125;, .... 总结： ​ 从effect资源可以看出，抖音底层有一个类似游戏引擎的这样的渲染框架，然后通过脚本进行控制。功能也应该是挺齐全的，应该是轻量版的引擎，可是麻雀虽小，五脏俱全。 ​ 对于熟悉游戏引擎的人来说，这部分应该不难，跟游戏引擎模块类似 ​ 另外，如果想学习引擎这快，我的思路是多去看看一些流行的开源引擎，一开始模仿别人怎么写，然后不断的思考总结，慢慢的你就知道一个引擎应该包含哪些，以及怎么控制各个模块之间的交互。 ​ 比如我之前深入研究过OGRE，看过OSG，以及深入研究过THREEJS，同时我自己写的渲染引擎，有把这几个引擎比较好的模块模仿过来，逐渐内化成自己的引擎。 ​ 后面有机会把拆解一下抖音比较复杂的特效吧，最好找一个和AI或者AR结合的特效(挖坑2) 3. 关于音视频图形部分​ 这里主要会大概整理一下比较基础的几个部分，包括滤镜、美颜、特效、转场这些，因为我目前好像只做过这些基础的玩法，更加复杂的就涉及到渲染引擎，还有AI图像方面，后面会继续学习研究 3.1 滤镜篇现在的滤镜主要还是查找表的形式包括1D/3D LUT，可能有些也会用AI去做，比如风格化迁移等 1D LUT: 调节亮度，对比度，黑白等级256 x 1，只影响Gamma曲线 RGB曲线调节256 x 3 Instagram 里面有很多1D的LUT应用，比如amaro, lomo, Hudson, Sierra….. 12345678910111213141516171819202122232425void main() &#123; vec2 uv = gl_FragCoord.st/u_resolution; uv.y = 1.0 - uv.y; vec4 originColor = texture2D(u_texture, uv); vec4 texel = texture2D(u_texture, uv); vec3 bbTexel = texture2D(u_blowoutTex, uv).rgb; //256x1 texel.r = texture2D(u_overlayTex, vec2(bbTexel.r, texel.r)).r; texel.g = texture2D(u_overlayTex, vec2(bbTexel.g, texel.g)).g; texel.b = clamp(texture2D(u_overlayTex, vec2(bbTexel.b, texel.b)).b, 0.1, 0.9); //256x3 vec4 mapped; mapped.r = texture2D(u_mapTex, vec2(texel.r, .25)).r; mapped.g = texture2D(u_mapTex, vec2(texel.g, .5)).g; mapped.b = texture2D(u_mapTex, vec2(texel.b, 0.1)).b; mapped.a = 1.0; mapped.rgb = mix(originColor.rgb, mapped.rgb, 1.0); gl_FragColor = mapped;&#125; Amaro 风格 Lomo 风格 …… 3D LUT: Lookup table : 64x64 512x512 12345678910111213141516171819202122232425262728293031323334353637383940//这没啥说的，都是很基本的void main() &#123; vec2 uv = gl_FragCoord.st/u_resolution; uv.y = 1.0 - uv.y; lowp vec3 textureColor = texture2D(u_texture, uv).rgb; textureColor = clamp((textureColor - vec3(u_levelBlack, u_levelBlack, u_levelBlack)) * u_levelRangeInv, 0.0, 1.0); textureColor.r = texture2D(u_grayTexture, vec2(textureColor.r, 0.5)).r; textureColor.g = texture2D(u_grayTexture, vec2(textureColor.g, 0.5)).g; textureColor.b = texture2D(u_grayTexture, vec2(textureColor.b, 0.5)).b; mediump float blueColor = textureColor.b * 15.0; mediump vec2 quad1; quad1.y = floor(blueColor / 4.0); quad1.x = floor(blueColor) - (quad1.y * 4.0); mediump vec2 quad2; quad2.y = floor(ceil(blueColor) / 4.0); quad2.x = ceil(blueColor) - (quad2.y * 4.0); highp vec2 texPos1; texPos1.x = (quad1.x * 0.25) + 0.5 / 64.0 + ((0.25 - 1.0 / 64.0) * textureColor.r); texPos1.y = (quad1.y * 0.25) + 0.5 / 64.0 + ((0.25 - 1.0 / 64.0) * textureColor.g); highp vec2 texPos2; texPos2.x = (quad2.x * 0.25) + 0.5 / 64.0 + ((0.25 - 1.0 / 64.0) * textureColor.r); texPos2.y = (quad2.y * 0.25) + 0.5 / 64.0 + ((0.25 - 1.0 / 64.0) * textureColor.g); lowp vec4 newColor1 = texture2D(u_lookupTexture, texPos1); lowp vec4 newColor2 = texture2D(u_lookupTexture, texPos2); lowp vec3 newColor = mix(newColor1.rgb, newColor2.rgb, fract(blueColor)); textureColor = mix(textureColor, newColor, u_strength); gl_FragColor = vec4(textureColor, 1.0); &#125; Fairytale风格 ​ 可以看到3D LUT看起来更加和谐一点，因为3D LUT 的RGB映射关系是关联在一起，1D LUT则是独立的，不过1D LUT可以节省空间，有些只需要单通道就可以搞定的，就可以用1D LUT ​ LUT得益于其简单的生产流程，基本设计师那边用PhotoShop调好一张查找表，这边就应用上去就OK了 相关资料： https://affinityspotlight.com/article/1d-vs-3d-luts/ https://zhuanlan.zhihu.com/p/37147849 https://zhuanlan.zhihu.com/p/60702944 3.2 美颜篇 磨皮去燥 最简单的方式，就是利用各种降噪滤波器，去掉高频噪声部分 均值模糊 权重分布平均 高斯模糊 权重高斯分布 表面模糊 权重分布只跟颜色空间有关系（与肤色检测配合） 双边滤波 权重分布跟距离和颜色空间分布有关系， 中值模糊 卷积核范围内去中值 导向滤波 参考图像 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667vec4 BilateralFilter(vec2 uv) &#123; float i = uv.x; float j = uv.y; float sigmaSSquare = 2.0 * SigmaS * SigmaS; float sigmaRSquare = 2.0 * SigmaR * SigmaR; vec3 centerColor = texture2D(u_texture, uv).rgb; float centerGray = Luminance(centerColor); vec3 sum_up, sum_down; for(int k = -u_radius; k &lt;= u_radius; k++) &#123; for(int l = -u_radius; l &lt;= u_radius; l++) &#123; vec2 uv_new = uv + vec2(k, l)/u_resolution; vec3 curColor = texture2D(u_texture, uv_new).rgb; float curGray = Luminance(curColor); vec3 deltaColor = curyolor - centerColor; float len = dot(deltaColor, deltaColor); float exponent = -((i-k)*(i-k)+(j-l)*(j-l))/sigmaSSquare - len/sigmaRSquare; float weight = exp(exponent); sum_up += curColor * weight; sum_down += weight; &#125; &#125; vec3 color = sum_up / sum_down; return vec4(color, 1.0);&#125;vec4 SurfaceFilter(vec2 uv) &#123; vec3 centerColor = texture2D(u_texture, uv).rgb; vec3 sum_up, sum_down; for(int k = -u_radius; k &lt;= u_radius; k++) &#123; for(int l = -u_radius; l &lt;= u_radius; l++) &#123; vec2 uv_new = uv + vec2(k, l)/u_resolution; vec3 curColor = texture2D(u_texture, uv_new).rgb; vec3 weight = CalculateWeight(curColor, centerColor); sum_up += weight * curColor; sum_down += weight; &#125; &#125; return vec4(sum_up / sum_down, 1.0); &#125;vec4 gaussian(vec2 uv, bool horizontalPass) &#123; float numBlurPixelsPerSide = float(blurSize / 2); vec2 blurMultiplyVec = 0 &lt; horizontalPass ? vec2(1.0, 0.0) : vec2(0.0, 1.0); //高斯函数 vec3 incrementalGaussian; incrementalGaussian.x = 1.0 / (sqrt(2.0 * pi) * sigma); incrementalGaussian.y = exp(-0.5 / (sigma * sigma)); incrementalGaussian.z = incrementalGaussian.y * incrementalGaussian.y; vec4 avgValue = vec4(0.0, 0.0, 0.0, 0.0); float coefficientSum = 0.0; avgValue += texture2D(texture, vertTexCoord.st) * incrementalGaussian.x; coefficientSum += incrementalGaussian.x; incrementalGaussian.xy *= incrementalGaussian.yz; for (float i = 1.0; i &lt;= numBlurPixelsPerSide; i++) &#123; avgValue += texture2D(texture, vertTexCoord.st - i * texOffset * blurMultiplyVec) * incrementalGaussian.x; avgValue += texture2D(texture, vertTexCoord.st + i * texOffset * blurMultiplyVec) * incrementalGaussian.x; coefficientSum += 2.0 * incrementalGaussian.x; incrementalGaussian.xy *= incrementalGaussian.yz; &#125; return avgValue / coefficientSum; 比如双边滤波的效果 高斯效果 suface blur 配合肤色检测，可以看出边缘部分有点硬， 2.美白 HighPass高亮加一点红晕 3.美型 美型主要是需要和人脸检测结合起来，对人脸各个部位进行微调，比如： 廋脸，根据AI找到的固定脸型的那几个关键点，在各自方向做曲线变形处理 大眼，找到中心点，放大一定的半径 下巴，也是做曲线变形处理 类似如下的曲线变形处理 123456789101112131415161718// 曲线形变处理vec2 curveWarp(vec2 textureCoord, vec2 originPosition, vec2 targetPosition, float radius)&#123; vec2 offset = vec2(0.0); vec2 result = vec2(0.0); vec2 direction = targetPosition - originPosition; float infect = distance(textureCoord, originPosition)/radius; infect = 1.0 - infect; infect = clamp(infect, 0.0, 1.0); offset = direction * infect; result = textureCoord - offset; return result;&#125; …… ​ 这一块主要是需要准确的关键点处理，然后就是怎么调整曲线了，可以自己注册一个Face++的FaceDetect自己玩一下，包括怎么和贴纸配合，这一块还在总结中，后续会自己研究(挖坑3) 3.3 转场篇​ 转场主要是涉及到两段视频之间，Shader会有两个Input，然后通过progress控制进度，我理解的转场主要包括以下这些： 1.UV变换 转场很多其实都是UV变化，UV坐标围绕某个点旋转、缩放、平移 1234567891011121314151617//scale centervec2 scale_uv = vec2(0.5 + (tc.x - 0.5) / scaleU , 0.5 + (tc.y - 0.5) / scaleV );//rotatevec2 rotateUV(vec2 uv, float rotation, vec2 mid)&#123; float ratio = (resolution.x / resolution.y); float s = sin ( rotation ); float c = cos ( rotation ); mat2 rotationMatrix = mat2( c, -s, s, c); vec2 coord = vec2((uv.x - mid.x) * ratio ,(uv.y -mid.y)*1.0); vec2 scaled = rotationMatrix * coord; return vec2(scaled.x / ratio + mid.x,scaled.y + mid.y);&#125;//translatevec2 translate_uv = vec2(0.5 + (tc.x - 0.5) / scaleU , 0.5 + (tc.y - 0.5) / scaleV ); 怎么控制进度就慢慢调吧 2.Blend转场 首先熟悉一下PhotoShop里面的混合模式alphe混合、滤色、加深、减淡、高亮度等等 https://zhuanlan.zhihu.com/p/23905865 然后就可以慢慢玩了 3.模糊转场 主要也是Photoshop里面的几种模糊方式，旋转模糊、高斯模糊、均值模糊等等 需要注意一下旋转模糊，我实现过一种旋转模糊的转场，需要配合随机采样 1234567891011121314151617float rand(vec2 uv)&#123; return fract(sin(dot(uv.xy ,vec2(12.9898,78.233))) * 43758.5453);&#125;vec4 rotation_blur(vec2 tc) &#123; angle = angle * PI_ROTATION / 180.0; vec2 uv = tc; float uv_random = rand(uv); vec4 sum_color = vec4(0.0); for(float i = 0.0; i &lt; samples; i++) &#123; float percent = (i + uv_random) / samples; float real_angle = angle + percent * strength; real_angle = mod(real_angle, PI_ROTATION); vec2 uv_rotation = rotateUV(uv, real_angle, center); sum_color += INPUT(fract(uv_rotation)); &#125; return sum_color / samples;&#125; And 这里有个转场的网站，可以研究一下 https://gl-transitions.com/ 3.4 特效篇基础的特效包括：抖动、灵魂出窍、故障风、光晕、老电影、粒子特效等等 比如old film 比如glitch风格(动态的会好一点) ​ 这些特效都不难，网上Shader到处都是，基本copy下来调调参数就行，可能也就粒子特效需要调很多细节，但如果你有图形引擎的基础，这些都很简单，到时候我可以建个仓库，share一些我自己实现的shader 一般特效的话可以先去网上找，比如shadertoy https://www.shadertoy.com/ 如果找不到的话，需要自己去想怎么实现，我一般是会按如下的方式去思考： 善于利用各种卷积滤波器(边缘检测、模糊)，有时候需要和随机采样结合 熟悉各种颜色空间，熟悉饱和度、锐度、亮度、色度等基础调节 UV变换多写几个有经验了，然后理解一些曲线函数，基本都能慢慢调出来 可以试着看看相关论文，或者OpenCV的实现方式 简单的特效实现起来并不难，多去写，慢慢总结经验的套路，如果更深入一点可能需要图形图像和数学的知识 3.5 串联这些效果​ 可能大家比较熟悉的就是GPUImage了，基本都是用这个去串联这些特效、滤镜之类，关于GPUImage网上也有很多资料，这里就不具体讲了，也比较简单，内部就是一个Input一个output通过FBO串起来。这里主要想推荐大家看下movit这个框架，基本跟GPUImage类似，支持单个input和多个input，可以打断中间节点，也可以有FrameBufferCache机制，但他还有一个优化的点就是，Shader动态组装机制，熟悉游戏引擎应该都知道这个，Shader是可以在最后动态生成，他里面是通过宏define来控制，可以把一些列串联特效组装到一起，非常高效，后面可以单独讲讲这块。 只需要看下他组装shader的过程： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546for (unsigned i = 0; i &lt; phase-&gt;effects.size(); ++i) &#123; Node *node = phase-&gt;effects[i]; const string effect_id = phase-&gt;effect_ids[make_pair(node, IN_SAME_PHASE)]; for (unsigned j = 0; j &lt; node-&gt;incoming_links.size(); ++j) &#123; if (node-&gt;incoming_links.size() == 1) &#123; frag_shader += "#define INPUT"; &#125; else &#123; char buf[256]; sprintf(buf, "#define INPUT%d", j + 1); frag_shader += buf; &#125; Node *input = node-&gt;incoming_links[j]; NodeLinkType link_type = node-&gt;incoming_link_type[j]; if (i != 0 &amp;&amp; input-&gt;effect-&gt;is_compute_shader() &amp;&amp; node-&gt;incoming_link_type[j] == IN_SAME_PHASE) &#123; // First effect after the compute shader reads the value // that cs_output() wrote to a global variable, // ignoring the tc (since all such effects have to be // strong one-to-one). frag_shader += "(tc) CS_OUTPUT_VAL\n"; &#125; else &#123; assert(phase-&gt;effect_ids.count(make_pair(input, link_type))); frag_shader += string(" ") + phase-&gt;effect_ids[make_pair(input, link_type)] + "\n"; &#125; &#125; frag_shader += "\n"; frag_shader += string("#define FUNCNAME ") + effect_id + "\n"; if (node-&gt;effect-&gt;is_compute_shader()) &#123; frag_shader += string("#define NORMALIZE_TEXTURE_COORDS(tc) ((tc) * ") + effect_id + "_inv_output_size + " + effect_id + "_output_texcoord_adjust)\n"; &#125; frag_shader += replace_prefix(node-&gt;effect-&gt;output_fragment_shader(), effect_id); frag_shader += "#undef FUNCNAME\n"; if (node-&gt;incoming_links.size() == 1) &#123; frag_shader += "#undef INPUT\n"; &#125; else &#123; for (unsigned j = 0; j &lt; node-&gt;incoming_links.size(); ++j) &#123; char buf[256]; sprintf(buf, "#undef INPUT%d\n", j + 1); frag_shader += buf; &#125; &#125; frag_shader += "\n";&#125; 完了之后可以生成类似如下的shader： 123456789101112131415161718192021222324252627282930313233343536373839404142434445precision highp float;varying vec2 tc;#define FUNCNAME eff0uniform sampler2D eff0_tex;vec4 FUNCNAME(vec2 tc) &#123; return texture2D(eff0_tex, vec2(tc.x,1.0-tc.y));&#125;#undef PREFIX#undef FUNCNAME#define INPUT eff0#define FUNCNAME eff1uniform float eff1_strength;uniform sampler2D eff1_lut;vec4 FUNCNAME(vec2 tc) &#123; float strength = eff1_strength; lowp vec4 textureColor = INPUT(tc); mediump float blueColor = textureColor.b * 63.0; mediump vec2 quad1; quad1.y = floor(floor(blueColor) / 8.0); quad1.x = floor(blueColor) - (quad1.y * 8.0); mediump vec2 quad2; quad2.y = floor(ceil(blueColor) / 8.0); quad2.x = ceil(blueColor) - (quad2.y * 8.0); highp vec2 texPos1; texPos1.x = (quad1.x * 0.125) + 0.5/512.0 + ((0.125 - 1.0/512.0) * textureColor.r); texPos1.y = (quad1.y * 0.125) + 0.5/512.0 + ((0.125 - 1.0/512.0) * textureColor.g); highp vec2 texPos2; texPos2.x = (quad2.x * 0.125) + 0.5/512.0 + ((0.125 - 1.0/512.0) * textureColor.r); texPos2.y = (quad2.y * 0.125) + 0.5/512.0 + ((0.125 - 1.0/512.0) * textureColor.g); lowp vec4 newColor1 = texture2D(eff1_lut, texPos1); lowp vec4 newColor2 = texture2D(eff1_lut, texPos2); lowp vec4 newColor = mix(newColor1, newColor2, fract(blueColor)); return mix(textureColor, vec4(newColor.rgb, textureColor.w), strength);&#125; #undef PREFIX#undef FUNCNAME#undef INPUT#define INPUT eff1void main()&#123; gl_FragColor = INPUT(tc);&#125; ​ 直接通过一个#define FUNCNAME就可以串起来，我以前写的那个渲染引擎，也是动态组装shader，跟这个有点类似，不过比这个复杂，需要更多的宏来控制，有兴趣可以去看看 Movit源码：https://git.sesse.net/?p=movit;a=summary 4. 关于音视频处理4.1 音视频理论知识 H264/H265编码原理，宏快怎么划分 I、P、B帧压缩方式 SPS/PPS 信息 音频的采样率 封装格式(MP4, FLV)，MP4的Box形式存储 YUV数据 4.2 编解码部分音视频解码基本流程 参考： https://blog.csdn.net/leixiaohua1020/article/details/18893769 1.硬解部分(Android MediaCodec) 在低端平台更多的需要依赖硬件解码，效率会更高，Android MediaCodec https://developer.android.com/reference/android/media/MediaCodec?hl=en 大概的代码逻辑 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667while (!m_sawOutputEOS) &#123; if (!m_sawInputEOS) &#123; // Feed more data to the decoder final int inputBufIndex = m_decoder.dequeueInputBuffer(TIMEOUT_USEC); if (inputBufIndex &gt;= 0) &#123; ByteBuffer inputBuf = m_decoderInputBuffers[inputBufIndex]; // Read the sample data into the ByteBuffer. This neither // respects nor // updates inputBuf's position, limit, etc. final int chunkSize = m_extractor.readSampleData(inputBuf, 0); if (m_verbose) Log.d(TAG, "input packet length: " + chunkSize + " time stamp: " + m_extractor.getSampleTime()); if (chunkSize &lt; 0) &#123; // End of stream -- send empty frame with EOS flag set. m_decoder.queueInputBuffer(inputBufIndex, 0, 0, 0L, MediaCodec.BUFFER_FLAG_END_OF_STREAM); m_sawInputEOS = true; if (m_verbose) Log.d(TAG, "Sent input EOS"); &#125; else &#123; if (m_extractor.getSampleTrackIndex() != m_videoTrackIndex) &#123; Log.w(TAG, "WEIRD: got sample from track " + m_extractor.getSampleTrackIndex() + ", expected " + m_videoTrackIndex); &#125; long presentationTimeUs = m_extractor.getSampleTime(); m_decoder.queueInputBuffer(inputBufIndex, 0, chunkSize, presentationTimeUs, 0); if (m_verbose) Log.d(TAG, "Submitted frame to decoder input buffer " + inputBufIndex + ", size=" + chunkSize); m_inputBufferQueued = true; ++m_pendingInputFrameCount; if (m_verbose) Log.d(TAG, "Pending input frame count increased: " + m_pendingInputFrameCount); m_extractor.advance(); m_extractorInOriginalState = false; &#125; &#125; else &#123; if (m_verbose) Log.d(TAG, "Input buffer not available"); &#125; &#125; final int decoderStatus = m_decoder.dequeueOutputBuffer(m_bufferInfo, dequeueTimeoutUs); if (decoderStatus == MediaCodec.INFO_TRY_AGAIN_LATER) &#123; // No output available yet if (m_verbose) Log.d(TAG, "No output from decoder available"); &#125; else if (decoderStatus == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) &#123; // Not important for us, since we're using Surface if (m_verbose) Log.d(TAG, "Decoder output buffers changed"); &#125; else if (decoderStatus == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) &#123; MediaFormat newFormat = m_decoder.getOutputFormat(); if (m_verbose) Log.d(TAG, "Decoder output format changed: " + newFormat); &#125; else if (decoderStatus &lt; 0) &#123; Log.e(TAG, "Unexpected result from decoder.dequeueOutputBuffer: " + decoderStatus); return ERROR_FAIL; &#125; else &#123; ///.... m_decoder.releaseOutputBuffer(decoderStatus, doRender); &#125; &#125; 解码过程网上到处都是，解码主要就是配合MediaExtrator，直接说一下需要注意的点： getOutputBuffer得到的数据格式主要是YUV420P和YUV420SP，UV通道排列不一样 seekTo完之后要Flush configure如果传入surface则是直接decode到suface上，不同通过getOutputBuffer获取数据 2.软解部分(FFMPEG) ​ 其实现在绝大部分音视频APP编解码都是用的FFMPEG，这里门也包含了硬解MediaCodec部分，所以只需要用FFMPEG就可以做硬解和软解的切换。关于FFMPEG的学习，首推肯定是雷神的博客啦， https://blog.csdn.net/leixiaohua1020/article/details/15811977 主要包括两个部分，首先是prepare部分， 1234567891011121314151617181920212223242526if (avformat_open_input(&amp;mFormatContext, path.c_str(), NULL, NULL) != 0) &#123; mlt_log_error(mProducer, "Could not open input file: %s", path.c_str()); goto error; &#125;if (avformat_find_stream_info(mFormatContext, NULL) &lt; 0) &#123; mlt_log_error(mProducer, "Could not find stream information"); goto error;&#125;mVideoStreamIndex = findPreferedVideoStream();//遍历track找到视频轨mVideoStream = mFormatContext-&gt;streams[mVideoStreamIndex];//然后就可以得到一些列参数，width，height，fps，fromat//如果是硬解//根据不同的编码器，找到对应的解码器，如H264，H265等mVideoCodec = avcodec_find_decoder_by_name("h264_mediacodec");;//如果是软解mVideoCodec = avcodec_find_decoder(mVideoStream-&gt;codecpar-&gt;codec_id);//openavcodec_open2(mVideoCodecContext, mVideoCodec, NULL)//就可以开始解码了 解码部分 1234567891011121314151617181920212223242526272829303132do &#123; if (!mPacket) &#123; ret = getVideoPacket(); if (ret &lt; 0 &amp;&amp; ret != AVERROR_EOF) &#123; break; &#125; &#125; //这里面送包的时候有很多细节 int ret = avcodec_send_packet(mVideoCodecContext, flush ? NULL : mPacket); if (ret &gt;= 0) freeVideoPacket(); else if (ret &lt; 0 &amp;&amp; ret != AVERROR_EOF &amp;&amp; ret != AVERROR(EAGAIN)) &#123; break; &#125; ret = avcodec_receive_frame(mVideoCodecContext, mFrame); if (ret &gt;= 0) &#123; ret = DECODER_FFMPEG_SUCCESS; //计算pts mCurrentPos = getTime(mFrame-&gt;pts == AV_NOPTS_VALUE ? mFrame-&gt;pkt_dts : mFrame-&gt;pts); pts = mCurrentPos; break; &#125; ++count; &#125; while (ret &lt; DECODER_FFMPEG_SUCCESS &amp;&amp; count &lt; DECODER_TRY_ATTEMPTS);//后面就是针对不同的格式，接入数据，//还可以通过swscaleFrame进行转码，转成你想要的格式//然后就是copy AVFrame里面的数据啦 关于FFMPEG有太多可以去学习的了，比如： 学会用ffmpeg ffprobe ffplay一些命令，真的非常强大，方便分析问题(ffprobe -i xxxx) 怎么去切换软解和硬码 熟悉API返回的状态 然后就是兼容性的问题： ​ 由于Android平台太复杂了，厂商很多，所以会有无穷无尽的兼容性问题需要去解决，总体来说兼容性问题主要包括： 数据源的兼容性，YUV格式非常多样，420p/420sp是常见的，还有yuv420p10le 这种10bit，贼坑 硬件平台的兼容性，这里坑就更多了，MediaCodec支持程度，尤其是低端机非常需要注意 分辨率问题，4K/8K，高通平台的支持程度 这个后续单独去整理吧……(挖坑4) 4.3 音视频同步视音频同步的实现方式其实有三种，分别是： 以音频为主时间轴作为同步源; 以视频为主时间轴作为同步源; 以外部时钟为主时间轴作为同步源; 具体用哪一种需要根据场景，比如在线视频播放器，一般会以第一种音频作为主时间轴去对齐视频帧数据，做丢帧和用当前帧处理，比如我之前写的音视频编辑SDK，因为我们有一条固定帧率的时间线，所以我们的对齐方式是以这条固定时间轴来对齐视频。 比如当前时间轴如果大于当前decode出来的帧的pts，就直接丢掉，继续找下一帧，如果当前时间轴小于decode出来的帧的pts，超过1一帧的时间就继续用上一帧渲染即可。 4.4 多视频多轨道(进阶) ​ 视频编辑SDK肯定是需要支持多视频多轨道编辑，如何高效的管理，方便编辑和预览，这里面主要是需要一个好的Multi track的框架支撑，在这里推荐一个MLT框架，这个框架对于多视频多轨道处理真的非常强大，内部有精确的时间戳对齐逻辑，只需要关注多轨道数据直接的排列，包括每一个视频片段生产数据，其内部会帮我们做好时间戳对齐，担任也可以和很多插件配合使用，比如movit，ffmpeg等。 ​ 基本框架 123+--------+ +------+ +--------+|Producer|--&gt;|Filter|--&gt;|Consumer|+--------+ +------+ +--------+ 具体的介绍后面单独写一篇博客整理，如下是官网介绍： https://github.com/mltframework/mlt 5. 关于传输这块没做过，对传输协议不太了解，只知道一些理论知识，比如RTMP的分块传输，后面有机会再补齐 5.1 RTMP6. 分析音视频APP​ 在开发过程中，对于一个小白来说可能会经常遇到一筹莫展的时候，这时候要学会向同类优秀的应用学习，比如做短视频相关的可以去看抖音/快手怎么做的，做音视频剪辑相关的可以看看见剪映/快影/小影怎么做的，你可以去把他们的包pull出来，比如我在做资源的接入的时候，就对比过快手/抖音/大疆 这几家的资源，看他们的sticker怎样接入，有png序列，有MP4，有自定义GIF格式的。所以从这些APP的packge内部还是能找到一些线索，跟着这些线索再慢慢找到其内部的逻辑，就比如我上面分析抖音潜水艇那个游戏一样，当然也可以去分析其它的一些文件，给你提供思路。 1. 抖音Package name: com.ss.android.ugc.aweme adb pull /data/data/com.ss.android.ugc.aweme 主要是分析了一下里面的Effect资源，还有LOG信息, 后面可以有机会继续拆解一下(挖坑5) 2. 快手Package name: com.smile.gifmaker 同样的方式 3. 大疆Package name: dji.mimo ​ 主要是DJI mimo，通过他们的sticker资源可以看到他们支持alpha通道视频的原理，后面有类似需求也可以拿过来用 7. 总结现在是凌晨3点多，从周日从早上开始写到晚上3点，差不多用了将近18个小时来整理[吐血]。想想上一次博客更新差不多是两年前了，那时候是做图形渲染相关的事情，也试着自己写了一个渲染引擎，结合了不错的开源引擎，写完之后对我图形方向的理解有很大的进步，于是试着开始写了几篇博客，继续维护自己关于图形方向的研究，后面由于各种原因，没有继续在维护之前的图形引擎，自己也不在更新博客了。 ​ 今天以音视频作为新的方向，重新回来，把自己这两年的一些积累整理出来，希望能对后面的学习者有所帮助，我也会尽力去打磨好每一篇博客，今天这遍博客只是总结一个大的框架，里面有非常非常多的细节，都可单独用一篇文章去讲，比如关于Camera，音视频特效，编解码，兼容性这些，都需要花时间去一点一点研究。我也会对自己每一篇文章有严格的要求，要么不写，要写就尽量按照论文的形式把每一个点讲清楚，结合流程图和效果图，最后还需要给出Demo复现效果。 ​ 还是开头那句话，随着5G是时代的到来，音视频的应用将会更加普及，这方面的人才也会更加紧缺，同时对于这里面的技术要求也会越来越高，从现在开始就慢慢积累，完善里面的技能栈，也可以选择一个分支去深入下去，这里面有很多的方向都值得研究。也希望以这篇文章为起点，寻找一些志同道合之人，一起去探索一些音视频方向的玩法，当然还有相机这块，因为我之前一直是做相机相关的，也思考过相机有什么好的方向可以去探索。 ​ 总之，我会一直做音视频这个方向，抖音和快手在视频玩法上做到了极致，手机厂商不断的升级Camera，也越来越重视视频的采集，让手机作为拍摄工具可以拍出更加震撼的效果，还有大疆在无人机领域视角去采集的视频数据，也可以做出很多大片效果，还有Insta360在全景方向的玩法，做的也很棒，未来随着硬件设备的升级，还会出现更多有趣的玩法，比如和AR/VR结合在一起，又会有怎样的火花呢，我们拭目以待。 人生只有一次，做自己喜欢的事吧]]></content>
      <categories>
        <category>音视频</category>
      </categories>
      <tags>
        <tag>图形图像</tag>
        <tag>音视频</tag>
        <tag>Camera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SimpleRenderEngineV1.0功能设计思想]]></title>
    <url>%2F2017%2F12%2F15%2F%EF%BC%88url%E4%B8%AD%E6%98%BE%E7%A4%BA%E7%9A%84%E6%A0%87%E9%A2%98%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1.SimpleRenderEngine初衷​ 到现在为止，我学习三维图形的时间将近有一年了，经过这一年的学习，也算是入门了，对图形学也有一定的理解，同时也爱上了图形算法的研究。因为自己是GIS专业的，所以对于三维图形方面的研发相比计算机专业的起步较晚，所以这一年感觉挺吃力的，因为有太多的知识需要学习强化，我重点放在了C++和图形方面的学习，当然这些都是工作之外的业余时间。大概三个月前我决定在学习图形学的过程中可以把所学到的东西整理到一个框架之下，写一个自己的渲染器，就开始了不断重构之路，写了之后才知道，其实难点根本不是图形算法方面，而是怎么去设计，才能把书上的这些图形的东西都放到一起，然后又方便扩展和调用。可能单独写一个demo很快就可以写出来（毕竟网上例子很多），比如实现模型加载、光照、阴影等，但怎么把这些所有的功能都集成到一起，这可能就是需要架构的经验，但我根本没有这方便的经验，咋办？这时候开源引擎然后加上自己的理解，慢慢的就有了一些思路，每次设计一个功能模块时，先去看一下开源的引擎是如何设计，怎么调用的，最好能把好的设计移植过来，比如刚刚整理的这个版本就大量参考了一些开源的设计，后面会仔细讲一些模块移植过程以及自己的一些理解。 ​ 其实在写的过程中我带着两个学习目的，其一当然是图形学的相关知识学习，其二就是渲染引擎或者C++工程方面框架设计的学习。所以写的时候我会假如自己在封装底层引擎功能，我如何设计才能让使用者更加容易理解，或者后期开发更加容易扩展和更新，且维护成本低，当然，由于缺乏经验，前期可能还是会以图形功能为主，然后尽量后面容易扩展和重构，尽量模仿开源引擎，因为扒开源代码也是一种很好的学习方式。所以这几个月大部分时间（都是工作之外的业余时间）都花在设计渲染器结构上，并没有实现出太多的图形功能，但我觉得在这个基础上，随着自己的深入研究，以后会有更多的功能集成上来。下面我就仔细介绍一下现有功能模块的一些设计。 2.基础库的一些介绍​ 既然打算自己写渲染器，当然我希望大部分模型都能自己写，哪怕去移植别人的代码也好，尽量少引用第三方库，这样也可以学到更多的东西。就比方说Math库，可能没几个人回去自己实现或者抄一遍，但我觉得我在写的过程中也学到了挺多东西，虽然自己可能已经熟悉一些矩阵变换操作，但自己写一遍真不一样，比如Matrix类，以前我可能对矩阵左乘右乘，行矩阵列矩阵理解有些模糊，但写完这个之后基本对矩阵的一些操作了然于心了，还有四元数也理解根深一些（尽管还没完全理解），还包括视图矩阵、投影矩阵的推导等等，这是写Math库的一些感悟。还有Light、Camera、Shader、Texture这些基础的就不说了，这里还想提的一个就是关于相机控制器，因为我工作内容有段时间就是在实现一套Google Earth的鼠标操作，所以还是花了点时间在相机控制方面，主要就是围绕相机姿态、位置还有插值这些知识的理解，还有定点旋转、第一人称、第三人称漫游等等，我可能会单独更新一篇博客关于如何写好相机控制器，现在这个版本也有很多没有移植出来。只是写了一套如何发送消息然后调用不同的Transform方法。还有一些零碎的模块由于还没测试或者还没写完所以就先不说了，后续会再更新。我直接贴一段目前这个版本的调用逻辑吧。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596Scene::ptr createScene()&#123; Scene::ptr scene = std::make_shared&lt;Scene&gt;(); //texture Texture::ptr earthTex = TextureManager::Inst()-&gt;loadTexture("earth", "../../../src/Data/texture/earthmap.jpg"); Texture::ptr boxTex = TextureManager::Inst()-&gt;loadTexture("cloud", "../../../src/Data/texture/box.jpg"); Texture::ptr floorTex = TextureManager::Inst()-&gt;loadTexture("earth", "../../../src/Data/texture/floor.jpg"); TextureUnitState::ptr earthUnit = std::make_shared&lt;TextureUnitState&gt;(); earthUnit-&gt;setTexture(earthTex); TextureUnitState::ptr boxUnit = std::make_shared&lt;TextureUnitState&gt;(); boxUnit-&gt;setTexture(boxTex); TextureUnitState::ptr floorUnit = std::make_shared&lt;TextureUnitState&gt;(); floorUnit-&gt;setTexture(floorTex); //material Material::ptr floorMat = std::make_shared&lt;Material&gt;(); floorMat-&gt;setMaterialType(Material::PhongMaterial); floorMat-&gt;setMap(floorUnit); floorMat-&gt;setCullFaceMode(CullFaceMode::DoubleSide); Material::ptr earthMat = std::make_shared&lt;Material&gt;(); earthMat-&gt;setMap(earthUnit); earthMat-&gt;setMaterialType(Material::PhongMaterial); Material::ptr boxMat = std::make_shared&lt;Material&gt;(); boxMat-&gt;setMaterialType(Material::PhongMaterial); boxMat-&gt;setMap(boxUnit); //mesh Mesh* floor = GeometryFactory::MakeQuad(100, 100); floor-&gt;setMaterial(floorMat); Mesh* box1 = GeometryFactory::MakeBox(20.0, 20.0, 20.0); box1-&gt;setPosition(Vector3D(-25.0, 10.0, 25.0)); box1-&gt;setMaterial(boxMat); Mesh* box2 = GeometryFactory::MakeBox(10.0, 10.0, 10.0); box2-&gt;setPosition(Vector3D(0.0, 5.0, -25.0)); box2-&gt;setMaterial(boxMat); Mesh* box3 = GeometryFactory::MakeBox(10.0, 10.0, 10.0); box3-&gt;setPosition(Vector3D(25.0, 5.0, 25.0)); box3-&gt;setMaterial(boxMat); Mesh* sphere = GeometryFactory::MakeSphere(8.0, 32, 32); sphere-&gt;setPosition(Vector3D(0.0, 8.0, 0.0)); sphere-&gt;setMaterial(earthMat); Object::ptr root = std::make_shared&lt;Object&gt;(); //light DirectionLight* dlight = new DirectionLight(); dlight-&gt;setPosition(Vector3D(0.0, 50.0, 50.0)); dlight-&gt;setShadowCamera(new OrthographicCamera(-50.0, 50.0, -50.0, 50.0, 0.1, 100.0)); PointLight* plight = new PointLight(); plight-&gt;setPosition(Vector3D(0.0, 50.0, 0.0)); plight-&gt;setShadowCamera(new PerspectiveCamera(MathHelper::radian(70.0), 1.0, 1.0, 200.0)); SpotLight* spotlight = new SpotLight; spotlight-&gt;setPosition(Vector3D(0.0, 30.0, 0.0)); spotlight-&gt;setAngle(M_PI/6.0); spotlight-&gt;setDecay(1.0); spotlight-&gt;setDistance(00.0); spotlight-&gt;setPenumbra(0.05); spotlight-&gt;setShadowCamera(new PerspectiveCamera(MathHelper::radian(50.0), 1.0, 1.0, 200.0)); root-&gt;add(floor); root-&gt;add(box1); root-&gt;add(box2); root-&gt;add(box3); root-&gt;add(sphere); root-&gt;add(dlight); root-&gt;add(plight); root-&gt;add(spotlight); scene-&gt;setSceneRoot(root); scene-&gt;setUseShadowMap(true); return scene;&#125;void main()&#123; Win::getSingleton()-&gt;create(); PerspectiveCamera::ptr camera = make_shared&lt;PerspectiveCamera&gt;(MathHelper::radian(65.0), (float)SCR_WIDTH / (float)SCR_HEIGHT, 0.1, 500.0); camera-&gt;setPosition(Vector3D(0.0f, 50.0f, -50.0)); camera-&gt;lookAt(0.0, 0.0, 0.0); Scene::ptr scene = createScene(); RenderSystem *rs = new RenderSystem(scene.get(), camera.get()); Win::getSingleton()-&gt;loadRenderSystem(rs); Win::getSingleton()-&gt;startRenderLoop(); delete rs;&#125; ​ ​ 这是一个简单的往Scene里面添加mesh和light，然后把scene和camera送到rendersystem开始渲染循环。 3.开源移植模块介绍​ 其实如果熟悉一些开源引擎，比如OSG、OGRE就立马看出很多东西都是从上面移植过来，比如完全移植了OGRE的HardwareBuffer，因为一开始看到这个就很酷，就是居然可以利用GPU来管理顶点数据，同时也可以在CPU上读写数据，我记得以前顶点数据我都是直接用vector容器管理，简直了。其实移植的过程就是理解或者是学习的过程，并不多单纯的复制粘贴，毕竟你没看到是不可能完全移植的，所以还是费了一点时间去理解它那套逻辑。这套东西可以管理顶点、索引和纹理数据，比HardwareVertexBuffer设计 1234567891011121314151617181920212223242526272829303132333435363738class HardwareVertexBuffer : public HardwareBuffer &#123; public: typedef std::shared_ptr&lt;HardwareVertexBuffer&gt; ptr; HardwareVertexBuffer(size_t vertex_size, size_t num_vertices, HardwareBuffer::Usage usage, bool use_shadow_buffer = false); ~HardwareVertexBuffer(); public: virtual void* lock(size_t offset, size_t length, LockOptions options); virtual void* lock(LockOptions options); virtual void unlock(void); virtual void readData(size_t offset, size_t length, void* dest); virtual void writeData(size_t offset, size_t length, const void* source, bool discardWholeBuffer = false); //virtual void copyData(HardwareBuffer&amp; src_buffer, size_t src_offset, size_t dst_offset, size_t length, bool discardWholeBuffer = false); //virtual void copyData(HardwareBuffer&amp; src_buffer); virtual size_t getSizeInBytes(void) const &#123; return _sizeInBytes; &#125; virtual Usage getUsage(void) const &#123; return _usage; &#125; virtual bool isLocked(void) const &#123; return _isLocked; &#125; virtual bool isUseShadowBuffer(void) const &#123; return _useShadowBuffer; &#125; virtual void upload(void); GLuint getBufferID() &#123; return _verexBufferID; &#125; unsigned int getVertexSize() &#123; return _vertexSize; &#125; unsigned int getVertexNum() &#123; return _numVertices; &#125; protected: GLuint _verexBufferID;//vbo size_t _numVertices;// size_t _vertexSize; size_t _sizeInBytes; Usage _usage; bool _isLocked; bool _useShadowBuffer; unsigned char* _data; &#125; ​ 它可以锁住一块内存，这块内存既可以是GPU也可以是CPU，然后进行读写数据，同时还可以设置shadowBuffer，是否在CPU端保存一份数据，比如在实际用的时候如下： 1234567891011121314151617181920212223242526272829303132333435363738394041Mesh* mesh = new Mesh;VertexData* vertexdata = new VertexData;vertexdata-&gt;setVertexStart(0);vertexdata-&gt;setVertexCount(4);VertexDeclaration::ptr vd = vertexdata-&gt;getVertexDeclaration();VertexBufferBinding::ptr bind = vertexdata-&gt;getVertexBufferBinding();size_t offset = 0;VertexElement::ptr tmp_ve = vd-&gt;addElement(0, offset, VET_FLOAT3, VES_POSITION);offset += tmp_ve-&gt;getTypeSize(VET_FLOAT3);tmp_ve = vd-&gt;addElement(0, offset, VET_FLOAT3, VES_NORMAL);offset += tmp_ve-&gt;getTypeSize(VET_FLOAT3);tmp_ve = vd-&gt;addElement(0, offset, VET_FLOAT2, VES_TEXTURE_COORDINATES);offset += tmp_ve-&gt;getTypeSize(VET_FLOAT2);char* data = (char*)malloc(sizeof(char)*vd-&gt;getVertexSize(0) * 4);float halfWidth = width / 2.0, halfHeight = height / 2.0;float vertices[32] = &#123; -halfWidth, 0.0, halfHeight, 0.0, 1.0, 0.0, 0.0, 0.0, halfWidth, 0.0, halfHeight, 0.0, 1.0, 0.0, 0.0, 1.0, -halfWidth, 0.0, -halfHeight, 0.0, 1.0, 0.0, 1.0, 0.0, halfWidth, 0.0, -halfHeight, 0.0, 1.0, 0.0, 1.0, 1.0&#125;;HardwareVertexBuffer* buffer = new HardwareVertexBuffer(offset, 4, HardwareBuffer::HBU_STATIC_WRITE_ONLY);bind-&gt;setBinding(0, (HardwareVertexBuffer::ptr)buffer);buffer-&gt;writeData(0, buffer-&gt;getSizeInBytes(), vertices);IndexData* indexdata = new IndexData;indexdata-&gt;setIndexStart(0);indexdata-&gt;setIndexCount(6);HardwareIndexBuffer * index_buffer = new HardwareIndexBuffer(HardwareIndexBuffer::IT_16BIT, 6, HardwareBuffer::HBU_STATIC_WRITE_ONLY);indexdata-&gt;setHardwareIndexBuffer((HardwareIndexBuffer::ptr)index_buffer);unsigned short faces[36] = &#123; 0,1,2, 2,1,3,&#125;;index_buffer-&gt;writeData(0, index_buffer-&gt;getSizeInBytes(), faces);mesh-&gt;setVertexData((VertexData::ptr)vertexdata);mesh-&gt;setIndexData((IndexData::ptr)indexdata);return mesh; ​ 可能这里与OGRE不同的地方是，我只是设置了一个简单的内存池去分割内存，OGRE的内存池管理不一样，具体看代码。 ​ 第二个借鉴的开源引擎就是工作中项目在用Threejs，它是WebGL版的开源库，当然它本身也有很多是借鉴OGRE的。我觉得Threejs之所以这么多人用就是它设计的非常好用，经常看到对图形学一点都不懂的在用Threejs，所以我就像我写的OpenGL版本能不能也这样设计，所以我借鉴了两个非常重要的设计，其一是它所有的对象都继承自Object3D，并且可以以节点的形式挂接，于是我设计了同样的类Object。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100class Mesh;class Light; class Sprite;class BillboardCollection;class ParticleSystem;class RayCaster;class TerrianTile;class Object &#123;public: typedef std::shared_ptr&lt;Object&gt; ptr; typedef std::vector&lt;Object::ptr&gt; Children; Object() :_position(Vector3D(0.0, 0.0, 0.0)), _scale(Vector3D(1.0, 1.0, 1.0)) &#123;&#125; Object(const Vector3D&amp; pos) :_position(pos), _scale(Vector3D(1.0, 1.0, 1.0)) &#123;&#125; Object(const Vector3D&amp; pos, const Quaternion&amp; quat) :_position(pos), _orientation(quat), _scale(Vector3D(1.0, 1.0, 1.0)) &#123;&#125;public: virtual Mesh* asMesh() &#123; return NULL; &#125; virtual const Mesh* asMesh() const &#123; return NULL; &#125; virtual Light* asLight() &#123; return NULL; &#125; virtual const Light* asLight() const &#123; return NULL; &#125; //virtual Plugin* asPlugin() &#123; return 0; &#125; //virtual const Plugin* asPlugin() const &#123; return 0; &#125; virtual Sprite* asSprite() &#123; return NULL; &#125; virtual const Sprite* asSprite() const &#123; return NULL; &#125; virtual BillboardCollection* asBillboardCollection() &#123; return NULL; &#125; virtual const BillboardCollection* asBillboardCollection() const &#123; return NULL; &#125; virtual ParticleSystem* asParticleSystem() &#123; return NULL; &#125; virtual const ParticleSystem* asParticleSystem() const &#123; return NULL; &#125; virtual TerrianTile* asTerrianTile() &#123; return NULL; &#125; virtual const TerrianTile* asTerrianTile() const &#123; return NULL; &#125; virtual Object* asObject() &#123; return this; &#125; virtual const Object* asObject() const &#123; return this; &#125; virtual void raycast(RayCaster* raycaster, Utils::AnyValue&amp; intersects) &#123; &#125;public: inline Object* getParent(); inline const Object* getParent() const &#123; return _parent; &#125; inline void setParent(Object* object) &#123; _parent = object; &#125; inline const Vector3D&amp; getPosition()const &#123; return _position; &#125; inline void setPosition(const Vector3D&amp; pos) &#123; _position = pos; &#125; inline const Vector3D&amp; getScale()const &#123; return _scale; &#125; inline void setScale(const Vector3D&amp; scale) &#123; _scale = scale; &#125; inline const Quaternion&amp; getOrientation()const &#123; return _orientation; &#125; inline void setOrientation(const Quaternion&amp; orientation) &#123; _orientation = orientation; &#125; void applyMatrix(const Matrix4D&amp; matrix); //_orientation set void setRotationFromAxisAngle(const Vector3D&amp; axis, double angle); void setRotationFromMatrix(const Matrix4D&amp; rotate); //_orientation change void rotateOnAxis(const Vector3D&amp; axis, double angle); void rotateOnAxisFixedPosition(const Vector3D&amp; axis, double angle);//special use void rotateOnX(double angle) &#123; rotateOnAxis(Vector3D(1.0, 0.0, 0.0), angle); &#125; void rotateOnY(double angle) &#123; rotateOnAxis(Vector3D(0.0, 1.0, 0.0), angle); &#125; void rotateOnZ(double angle) &#123; rotateOnAxis(Vector3D(0.0, 0.0, 1.0), angle); &#125; //for position change void translateOnAxis(const Vector3D&amp; axis, double distance); void translateOnX(double distance) &#123; translateOnAxis(Vector3D(1.0, 0.0, 0.0), distance); &#125; void translateOnY(double distance) &#123; translateOnAxis(Vector3D(0.0, 1.0, 0.0), distance); &#125; void translateOnZ(double distance) &#123; translateOnAxis(Vector3D(0.0, 0.0, 1.0), distance); &#125; Vector3D getDirection()const &#123; return _orientation * Vector3D(0, 0, 1); &#125; Vector3D getUp(void) const &#123; return _orientation * Vector3D(0, 1, 0); &#125; Vector3D getRight(void) const &#123; return _orientation * Vector3D(1, 0, 0); &#125; //useful function void localToWorld(Vector3D&amp; vector); void worldToLocal(Vector3D&amp; vector); // bool add(Object* object); bool remove(Object* object); Object::ptr getChild(int index); unsigned int getChildCount()const &#123; return _children.size(); &#125; //Children getChildren() &#123; return _children; &#125; void updateMatrixLocal(); void updateMatrixWorld(); // Matrix4D getLocalMatrix(); Matrix4D getWorldMatrix(); Vector3D getWorldPosition(); Quaternion getWorldQuaternion(); Vector3D getWorldScale();protected: Object* _parent; Vector3D _position; Vector3D _scale; Quaternion _orientation; Children _children; Matrix4D _matrix_local; Matrix4D _matrix_world;&#125;; ​ 所有的Actor（UE4里面叫做Actor），我觉得非常合适，都有这些通用的属性信息，并且同时有OSG节点的设计，可以很好的连接场景，渲染的时候从根节点开始更新数据（主要是更新矩阵信息），然后不同的类型对场景又有不同的作用。Scene只需要记录根节点信息即可，具体见代码。 ​ 其二移植的地方就是Material，Threejs的不同的Material可以组装成不同的shader，从而得到不同的效果，目前移植了BasicMaterial和PhongMaterial，一个没有光照，一个有光照。通过Material设置属性，然后再渲染的时候根据材质类型获取需要的属性信息给uniform赋值，感觉这样的设计很不错，同时也移植了Threejs的BRDF的光照模型，还没测试GGX的那套，只测试了简单的，感觉效果可以，关于物理光照，后期会整理一篇文章，因为最近也在看这个。对于Material的移植可能后期还会详细讲述，因为目前还没完全移植，比如PhongMaterial加了光照效果如下。 更多实际效果以及具体代码看Github。 ​ 这两块是大块的移植，当然还有其它一些小的地方，就不一而足了。其实本来想把OGRE的ResourceManager这套移植过来，后来好像没啃下来，主要是感觉对我目前的这个版本实用性不大，至多就是有个TextureManager或者ShaderManager，如果后期会有多个场景的概念，再移植它这种资源的加载卸载机制来管理不同场景的资源。 4.最后​ 说实话写的时候一直在问自己一个问题，这有意义吗？别人开源引擎都封装的很好，用好它就可以做出很炫的效果，而且时间成本很低。的确如此，这可能并不是最有效率的学习方式，但却是我最喜欢的学习方式。我希望自己能有一件自己感兴趣的事情，一直去坚持写，坚持研究下去，仅仅是因为喜欢，不去追求它的实际意义。而些SimpleRenderEngine可能就是这么一件事情，我喜欢写代码，喜欢做一些酷炫的图形效果，然后就有了这件事，这可能就是它的意义吧，就是因为喜欢。 ​ 最后希望志同道合之人加入QQ群（528379336）一起讨论，同时欢迎fork Github地址：https://github.com/LukiYLS/SimpleRenderEngine]]></content>
      <categories>
        <category>SimpleRenderEngine</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>OpenGL</tag>
        <tag>OSG</tag>
        <tag>OGRE</tag>
        <tag>ThreeJS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始写渲染引擎-开篇]]></title>
    <url>%2F2017%2F09%2F05%2F%EF%BC%88url%E4%B8%AD%E6%98%BE%E7%A4%BA%E7%9A%84%E6%A0%87%E9%A2%98%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言​ 最近 准备开始写一个自己的渲染引擎，主要有两个目的：一是为了系统的学习关于如何从底层开始写一个渲染引擎，二是为了以后方便给自己用；我会在博客和github上同步更新我的进度，博客的目的是为了把自己开发过程中的思路记录下来，同时也希望能找到志同道合之人一起学习进步，如果自己的经验能够给别人有点帮助那就最好不过了，同时所有的代码都会更新到github当中，如果有人能加入一起造轮子，那我会非常欢迎。 ​ 其实写之前也犹豫了很久，很多人可能会问，现在这么多开源的引擎，为什么不直接用呢，原因其实很简单，就像我之前讲过我对开源的理解，开源并不是说我们能用它做多少事情，而是我们能从开源当前学会多少东西。现在越来越少的人会去自己造轮子，都是直接用现成的东西，所以经常会遇到很多半桶水水平的人，这种人真多很烦，因为我是GIS专业的，而且也是在GIS公司，公司有些所谓的 老鸟，用过一些开源的引擎做了点东西，以为自己对图形学很熟悉了，可是经常连模板测试都一知半解的，然后还跟你争执，真是无语了，这让我更加意识到自己需要把基础打好，不能步其后尘。而造轮子是非常好的学习方法，很多东西真的需要自己动手去一个坑一个坑的跳，你才能对一些知识理解的更深入一点。就比方说学习编译原理最好的方法就是自己写个编译器。 ​ 我可能会先从OpenGL开始写起，后面会考虑慢慢更新WebGL版本，不过WebGL版本可能会倾向于数据可视化方面，因为这也是我想研究的一个方向，现在可能自己还需要学习一下，毕竟现在Web端也算是一个趋势了，不过两个版本如果用同一个架构的话，移植还是挺好的，所以，目前还是先把OpenGL版本写好吧！ SimpleRenderEngine V1.0框架设计​ 其实我写这篇博客的时候已经完成了一些基础的框架设计，主要是实现了一些基础类，比如Texture，Light，Camera，Entity，Scene，Mesh我等等。要说框架设计，好像没什么东西，就是先想一个简单的方法把它们串到一起，后面可能要慢慢重构吧，因为第一版本没考虑很高深的架构设计，但我写之前对自己引擎框架的要求就是，外部尽量简单，所以就需要里面能很好的连在一起，因为现在模块也比较少，所以把它们联系到一起很简单。先看下我这个引擎绘制一个cube外部代码大概向下面这样： 12345678910111213141516171819202122232425262728293031Win::Inst()-&gt;createWindow(); vector&lt;Vertex&gt; vertices;vertices.push_back(Vertex(0.5f, 0.5f, 0.0f, 0, 0, 1, 1, 1));vertices.push_back(Vertex(0.5f, -0.5f, 0.0f, 0, 0, 1, 1, 0));vertices.push_back(Vertex(-0.5f, -0.5f, 0.0f, 0, 0, 1, 0, 0));vertices.push_back(Vertex(-0.5f, 0.5f, 0.0f, 0, 0, 1, 0, 1));vector&lt;unsigned int&gt; indices = &#123; 0, 3, 1, 1, 3, 2 &#125;;Mesh::ptr mesh = std::make_shared&lt;Mesh&gt;();mesh-&gt;setVertices(vertices);mesh-&gt;setIndex(indices); mesh-&gt;createBuffer(); Light::ptr light = std::make_shared&lt;Light&gt;();light-&gt;setType(PointLight);TextureManager::Inst()-&gt;loadTexture("../../../src/Data/texture/1.jpg", "texture1");TextureManager::Inst()-&gt;loadTexture("../../../src/Data/texture/2.jpg", "texture2");TextureManager::Inst()-&gt;loadTexture("../../../src/Data/texture/3.jpg", "texture3");Camera::ptr camera = std::make_shared&lt;Camera&gt;(glm::vec3(0.0f, 0.0f, 3.0f));camera-&gt;setPerspectiveFovLHMatrix(glm::radians(45.0f), (float)SCR_WIDTH / (float)SCR_HEIGHT, 0.1f, 100.0f);Shader::ptr shader = std::make_shared&lt;Shader&gt;("../../../src/Data/shader/basic.vs", "../../../src/Data/shader/basic.fs"); mesh-&gt;setProgram(shader); mesh-&gt;addTexture("texture3");Scene::Inst()-&gt;addEntity("test", (Entity::ptr)mesh);Scene::Inst()-&gt;addLight(light);Win::Inst()-&gt;starup(camera); ​ 其实这个很简单，就是数据丢到mesh当中，纹理丢到textureManager中，然后建个camera，把mesh加到scene中，绘制，OK。具体内部怎么调用，大家可以到github把代码git下来看一下。 ​ 我可能不会在博客里将太多理论的东西，因为红宝书上都有，如果有个别不能理解的可以提问，可能后续会有些原理复杂一点的我会专门写篇博客去讲解，我会把思路，其实应该是框架性的东西，分享出来，当我写的过程中不知怎么办的时候，我也会写出来跟大家交流。我是个小白，所以也会有很多问题，希望能得到大牛的指点，也希望自己能坚持下去，只是因为喜欢。]]></content>
      <categories>
        <category>从零开始写渲染引擎</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>OpenGL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Earth开发原理-基于LRU-K实现瓦片数据析构算法]]></title>
    <url>%2F2017%2F09%2F02%2F%EF%BC%88url%E4%B8%AD%E6%98%BE%E7%A4%BA%E7%9A%84%E6%A0%87%E9%A2%98%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 用户访问三维earth过程中，客户端会源源不断的向服务器请求数据，尤其是多用户并发的情况下，请求更加频繁，所以服务器通常会设置缓存容器，主要是DEM和DOM缓存池，数据从server中请求后，会在服务器同时保留一份。比如请求google 影像，服务器根据wtms服务地址以及用户请求的瓦片层级，然后利用HTTP协议将数据传输至服务器，这时候服务器就将数据进行备份，这样等下一次在请求同样的数据时，就可以直接从服务器中获取，这样可以提高用户访问效率。然而，随着用户访问的增加，服务器缓存压力必然也会加大，所以就需要及时析构一些数据，保证服务器缓存池不会爆。 LRU和LRU-K算法概述​ 缓存淘汰算法包括LRU、LFU和FIFO，关于这三种算法的比较，可以看一这篇博客： 缓存算法（页面置换算法）-FIFO、LFU、LRU ​ 这里只对LRU-K（LRU-K改进）算法作一个简单介绍，LRU或者LRU-K缓存析构算法是一种比较简单常用的缓存淘汰机制，LRU主要根据资源访问时间来淘汰数据，也就是说访问数据间隔越久，其淘汰顺序应该是月优先，所以在设计算法时，只需要设计一个队列来存储每个资源的访问时间，新访问的数据放到最前，这样就可以保证末尾的数据时间间隔最久，每次淘汰只需从末尾开始淘汰。 ​ 而LRU-K，则是增加了一个访问次数变量，通过访问次数K来控制需要淘汰的数据，所以它需要多维护一个队列来记录访问次数， 数据第一次被访问，加入到访问历史列表； 如果数据在访问历史列表里后没有达到K次访问，则按照一定规则（FIFO，LRU）淘汰； 当访问历史队列中的数据访问次数达到K次后，将数据索引从历史队列删除，将数据移到缓存队列中，并缓存此数据，缓存队列重新按照时间排序； 缓存数据队列中被再次访问后，重新排序； 需要淘汰数据时，淘汰缓存队列中排在末尾的数据，即：淘汰“倒数第K次访问离现在最久”的数据。 瓦片数据析构算法实现​ LRU-K淘汰机制可以很好的应用到瓦片数据的析构中，因为访问次数和时间两个权重可以通时考虑到用户访问的效率以及缓存池的压力，所以考虑将此方法应用到DEM和DOM缓冲析构当中，首先要考虑的是两个参数的确定，时间当然是指用户的访问某瓦片的时间，次数表示某瓦片被用户访问的次数，每个用户只会记录一次，这里需要对LRU-K作一个小小的改进，因为我们考虑析构的原则是：很久没被访问，且访问次数少的瓦片应该优先淘汰，所以需要把这两个权重都加入到用户访问记录中去。 ​ 综合考虑，析构算法主要思路是：当用户请求瓦片时，记录该瓦片的访问时间，并且增加瓦片访问次数（+1操作）。每次请求数据时，通过时间来更新访问次数，如果访问时间间隔大于给定的最大时间，则访问次数（-1操作）。通过+1和-1两个操作控制瓦片访问次数，当内存池容量超过一定阈值，需要执行析构时，从访问记录中取前K个访问次数较少的瓦片，释放其资源。 ​ LRU Node的设计如下： 1234567891011121314151617class lru_node &#123; friend class lru_queue; public: lru_node(); virtual ~lru_node(); public: void Hit(unsigned int time_now_); unsigned int get_time(); void bind_queue(lru_queue* queue_); void unbind_queue(); protected: unsigned int _last_use_time; lru_queue* _lru_queue; std::list&lt;lru_node*&gt;::iterator _self; &#125;; ​ 可以看出，这里只是设计了节点的访问时间，并且维护了一个队列，lru_queu的设计如下： 12345678910111213141516171819202122232425262728class lru_queue &#123; friend class lru_node; public: typedef gw_shared_ptr&lt;lru_queue&gt; ptr; public: lru_queue(runtime* run_time_, terrian_dem_resource_manager* dem_manager_); lru_queue(runtime* run_time_, terrian_dom_resource_manager* dom_manager_); ~lru_queue(); public: void move_top(lru_node* node_); void remove(lru_node* node_); unsigned int get_time(); lru_node* pop(); int get_queue_size(); void update(unsigned int lost_time_); protected: std::list&lt;lru_node* &gt; _list_queue; private: gw::mutex::ptr _queue_mutex; //union //&#123; enum Type&#123;DEM,DOM&#125;_type; terrian_dem_resource_manager* _dem_manager; terrian_dom_resource_manager* _dom_manager; //&#125;; &#125;; ​ 其中update函数就是通过时间来更新瓦片访问次数，就是从尾部遍历队列，如果访问时间与当前时间间隔大于lost_time，则只需-1操作。 ​ +1操作如下： 123456789101112131415161718192021222324252627void add_user_visit_record(std::string userid, geocode gc) &#123; lru_tile* node = new lru_tile(); node-&gt;_gc = gc; //dom_rm-&gt;_dom_list-&gt;move_top(node); unsigned int timt_now = get_tick_count(); node-&gt;Hit(timt_now); _queue_service-&gt;move_top(node, userid); _visit_record_mutex-&gt;lock(); std::vector&lt;RECORD_TYPE&gt;::iterator it = _visit_record_vec.begin(); for (; it != _visit_record_vec.end(); ++it) &#123; if (it-&gt;first == gc) &#123; it-&gt;second++; break; &#125; &#125; if (it == _visit_record_vec.end()) &#123; RECORD_TYPE record = std::make_pair(gc,1); _visit_record_vec.push_back(record); &#125; _visit_record_mutex-&gt;unlock(); &#125; 与之对应的-1操作如下： 1234567891011121314void visit_count_plus_one(geocode gc_)&#123; _visit_record_mutex-&gt;lock(); std::vector&lt;RECORD_TYPE&gt;::iterator it = _visit_record_vec.begin(); for (; it != _visit_record_vec.end(); ++it) &#123; if (it-&gt;first == gc_) &#123; it-&gt;second--; break; &#125; &#125; _visit_record_mutex-&gt;unlock();&#125; 总结​ 这个析构算法，虽然到现在并没有真正测试多用户并发情况下的效率，只是可以保证内存池能够稳定在阈值之内，可以到实际情况，还需要不断的优化和改进，因为数据加载效率也是非常重要的考量。]]></content>
      <categories>
        <category>Earth开发原理</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Earth开发原理-阴影体实现要素贴地]]></title>
    <url>%2F2017%2F08%2F26%2F%EF%BC%88url%E4%B8%AD%E6%98%BE%E7%A4%BA%E7%9A%84%E6%A0%87%E9%A2%98%EF%BC%89%2F</url>
    <content type="text"><![CDATA[关于阴影实现方法概述​ Shadow map: 阴影的实现技术，我以前实现过比较基础的shadow map，就是利用从光源出发得到的深度图，然后每一个像素去比较当前深度值与深度图中的得到的深度值，深度图其实保存的应该是光源到遮光板之间的距离，而与它比较的是像素到光源的距离，如果小于，则说明处于阴影之中。还有一些细节的改进，比如阴影交叉如何处理（与深度图的分辨率有关，如果多个像素用同一个比较，就会出现这种情况），还有锯齿如何处理等等。 具体可以看这篇教程：Shadow map。 ​ Shadow volume: 上面这种方法会有两个问题：1.如果光源动态变化，则每一次shadow map都要更新，这样开销会很大；2.上述出现的交叉阴影和锯齿问题，都没有一种很好的方法完美解决。因此Franklin C. Crow在1977年 提出了一种新的方法：Shadow volume。不过这种方法好像对于多光源的情况下，也并无优势。所以现在主流的依然是shadow map方法，可以结合deferred lighting技术。不过因为今天的主题是要用到阴影体这种方法，所以下面线详细介绍一下这种方法。 Shadow volume实现方法​ shadow volume主要用到的原理是模板测试和深度测试，通过这两个测试，把阴影部分的stencil buffer计算出来，然后render这部分就可得到阴影。 ​ 第一步：构建阴影体。对mesh每一条边沿着光源方向进行拉伸，顶点部分除了原始的n个顶点(x,y,z,1)外，拉伸后的顶点同样也有n个，拉伸的长度应是无限远，但我们只有将其顶点形式设置为（x,y,z,0），在shader里面转换后自然就是无穷远处。面主要包括顶面、底面和侧面，底面三角形索引结构其实和顶面是一样的，侧面就是每一个quad由两个三角形组成，这样就可以得到阴影体的vertex buffer。 ​ 第二步：Render pass。 ​ pass1：打开depth test，按正常方式渲染整个场景，得到depth map。 ​ pass2：打开stencil test，关掉z writing和color buffer writing，渲染shadow volumes；设置stencil test always pass，对于front faces，若z test pass，则stencil value +1，若z test fail，则不更新stencil value；对于back faces，若z test pass，则stencil value -1。 ​ pass3：pass2完成之后，stencil buffer中value不为0的像素就处于阴影区域，据此绘制阴影效果即可。 12345678910111213141516//pass1RenderVolumetoDepth();//pass 2glDepthMask(GL_FALSE);glEnable(GL_DEPTH_CLAMP);glDisable(GL_CULL_FACE);glStencilFunc(GL_ALWAYS, 0, 0xff); glStencilOpSeparate(GL_BACK, GL_KEEP, GL_INCR_WRAP, GL_KEEP);glStencilOpSeparate(GL_FRONT, GL_KEEP, GL_DECR_WRAP, GL_KEEP);RenderVolume();//pass 3glStencilFunc(GL_EQUAL, 0x0, 0xFF); glStencilOpSeparate(GL_BACK, GL_KEEP, GL_KEEP, GL_KEEP);RenderVolume();glDisable(GL_STENCIL_TEST);...... ​ 这样就可以实现一个简单的阴影效果了，其中核心就是利用模板测试标记出阴影部分，当然还有一些细节，比如多光源的时候需要遍历每一个光源，想根深一步了解可以看《GPU Gems》Chapter 9. Efficient Shadow Volume Rendering. 利用阴影体实现贴地效果​ 前面简单的介绍了一下shadow volume的原理，下面要进入正题，要素贴地。这算是三维球中一个很基本的功能，因为这会涉及要标会，量测，矢量要素加载等等，所以这个问题也必须解决，才能有后续这些相关功能的实现。一开始对这个没什么概念，想了很多方法，第一反应到的就是纹理。在有DEM数据的情况下，面对高低起伏的地形，如何将要素随着地形起伏，感觉只有将其作为一张纹理贴地和瓦片数据一样贴到地形网格上，才能完美贴地，好像osgearth就是这样做的，所以就去研究投影纹理技术，投影纹理就不展开讲了，就类似虚拟一个投影仪，然后将纹理投影到物体表面。 ​ 阴影体这个方法是看了cesium的源码才知道原来贴地还可以这样，关于cesium后面应该还会有很多篇博客来讲其中的一些图形学知识，因为近期一直在研究它的源码，cesium算是web端的标杆了，类似pc端的oe，现在很多都在用它搞二次开发，比如超图。我们虽然没有直接用它，但是很多地方都在借鉴它的一些原理，其实我觉得这才是开源的意义，开源并不是让你可以直接用它进行二次开发，更主要的是你能够弄懂里面的一些原理，并为自己所用，这叫站在巨人的肩膀上。现在很多所谓的三维GIS创业公司，不过是做了点二次开发，包装了一个界面，就拿出来骗钱，非常可笑。不过也是，这几年政府的钱太好骗了，不过这样的没点技术积累的公司基本也存活不了多久。好了，不说废话了，开始装逼了。 ​ 前面说了阴影体怎么构建，那到了有DEM的情况下，又应该如何构建呢？比如说画一个Polygon，首先要做的就是拉伸，那拉伸又包括拉伸方向和拉伸高度，拉伸方向很简单就是顶点法线，拉伸高度应该与polygon所在地形高程与关，必须保持拉伸高度超过其范围之内的最大高程，所以拉伸高度就是高程的最大值。 ​ 阴影体构建之后，又如何只绘制贴地部分呢，这就要用到前面说的模板测试和深度测试了，这里有会有三个pass，不过这三个pass与前面的有点区别。 1234567891011121314151617181920212223242526272829//pass 1gl.colorMask(false,false,false,false);gl.depthMask(false);gl.enable(gl.STENCIL_TEST);gl.disable(gl.DEPTH_TEST);gl.disable(gl.CULL_FACE);gl.stencilFuncSeparate(gl.FRONT,gl.ALWAYS,0,~0);gl.stencilFuncSeparate(gl.BACK,gl.ALWAYS,0,~0);gl.stencilOpSeparate(gl.BACK,gl.KEEP,gl.DECR,gl.INCR_WRAP); gl.stencilOpSeparate(gl.FRONT,gl.KEEP,gl.DECR,gl.DECR_WRAP);renderShadowVolume(camera);//pass 2gl.enable(gl.DEPTH_TEST);gl.depthFunc(gl.LEQUAL);gl.stencilFuncSeparate(gl.FRONT,gl.ALWAYS,0,~0);gl.stencilFuncSeparate(gl.BACK,gl.ALWAYS,0,~0); gl.stencilOpSeparate(gl.BACK,gl.KEEP,gl.KEEP,gl.INCR_WRAP);gl.stencilOpSeparate(gl.FRONT,gl.KEEP,gl.KEEP,gl.DECR_WRAP);renderShadowVolume(camera);//pass 3gl.colorMask(true,true,true,true);gl.enable(gl.STENCIL_TEST);gl.enable(gl.BLEND);gl.disable(gl.DEPTH_TEST);gl.stencilFunc(gl.NOTEQUAL,0,~0);gl.stencilOp(gl.KEEP,gl.KEEP,gl.DECR);renderShadowVolume(camera);//set backgl.depthMask(true);gl.disable(gl.STENCIL_TEST); gl.enable(gl.CULL_FACE); gl.enable(gl.DEPTH_TEST); ​ pass 1：关闭颜色写入以及背面裁剪（因为是双面绘制，模板函数不一样），开启模板测试，关闭深度测试，正面模板值-1，背面模板值+1 ​ pass 2：开启深度测试和模板测试，同样正面-1，背面加+1 ​ pass 3：开启颜色写入，将模板值不为0的区域绘制出来，自然就是和地形相交的部分，就可以看到贴地效果。 ​ 这里有个很有意思的地方，就是只要pass 2和pass 3其实就可以标记出贴地部分，但如果你camera进入到阴影体区域，应该通过关闭深度测试来标记出把地形遮挡的区域，所以pass 1的目的其实是为了标记出进入阴影体内部的贴地区域，这两个标记需要分成两个pass，这样即使你进入阴影体内部也可以达到同样的效果。 总结​ 这算是我我开通博客站点后的第一篇博客，我就喜欢这种直接上干货的形式，毕竟这也是我想写博客的目的，积累和分享。同时也希望能保存博客的质量，只要是拿出来的，一定要有价值。这篇博客的分类是Earth开发原理，意味着以后我会陆续把我在开发三维earth过程中，一些有价值，有意思的技术分享出来，但我并不会直接把源码放出来，因为我分享的技术。也希望有看到的朋友多提建议，共同交流。 ​ 预告：下一篇可能会是google earth相机实现]]></content>
      <categories>
        <category>Earth开发原理</category>
      </categories>
      <tags>
        <tag>三维GIS</tag>
        <tag>图形算法</tag>
        <tag>WebGL</tag>
      </tags>
  </entry>
</search>
