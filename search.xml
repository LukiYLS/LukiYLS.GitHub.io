<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SimpleRenderEngineV1.0功能设计思想]]></title>
    <url>%2F2017%2F12%2F15%2F%EF%BC%88url%E4%B8%AD%E6%98%BE%E7%A4%BA%E7%9A%84%E6%A0%87%E9%A2%98%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1.SimpleRenderEngine初衷​ 到现在为止，我学习三维图形的时间将近有一年了，经过这一年的学习，也算是入门了，对图形学也有一定的理解，同时也爱上了图形算法的研究。因为自己是GIS专业的，所以对于三维图形方面的研发相比计算机专业的起步较晚，所以这一年感觉挺吃力的，因为有太多的知识需要学习强化，我重点放在了C++和图形方面的学习，当然这些都是工作之外的业余时间。大概三个月前我决定在学习图形学的过程中可以把所学到的东西整理到一个框架之下，写一个自己的渲染器，就开始了不断重构之路，写了之后才知道，其实难点根本不是图形算法方面，而是怎么去设计，才能把书上的这些图形的东西都放到一起，然后又方便扩展和调用。可能单独写一个demo很快就可以写出来（毕竟网上例子很多），比如实现模型加载、光照、阴影等，但怎么把这些所有的功能都集成到一起，这可能就是需要架构的经验，但我根本没有这方便的经验，咋办？这时候开源引擎然后加上自己的理解，慢慢的就有了一些思路，每次设计一个功能模块时，先去看一下开源的引擎是如何设计，怎么调用的，最好能把好的设计移植过来，比如刚刚整理的这个版本就大量参考了一些开源的设计，后面会仔细讲一些模块移植过程以及自己的一些理解。 ​ 其实在写的过程中我带着两个学习目的，其一当然是图形学的相关知识学习，其二就是渲染引擎或者C++工程方面框架设计的学习。所以写的时候我会假如自己在封装底层引擎功能，我如何设计才能让使用者更加容易理解，或者后期开发更加容易扩展和更新，且维护成本低，当然，由于缺乏经验，前期可能还是会以图形功能为主，然后尽量后面容易扩展和重构，尽量模仿开源引擎，因为扒开源代码也是一种很好的学习方式。所以这几个月大部分时间（都是工作之外的业余时间）都花在设计渲染器结构上，并没有实现出太多的图形功能，但我觉得在这个基础上，随着自己的深入研究，以后会有更多的功能集成上来。下面我就仔细介绍一下现有功能模块的一些设计。 2.基础库的一些介绍​ 既然打算自己写渲染器，当然我希望大部分模型都能自己写，哪怕去移植别人的代码也好，尽量少引用第三方库，这样也可以学到更多的东西。就比方说Math库，可能没几个人回去自己实现或者抄一遍，但我觉得我在写的过程中也学到了挺多东西，虽然自己可能已经熟悉一些矩阵变换操作，但自己写一遍真不一样，比如Matrix类，以前我可能对矩阵左乘右乘，行矩阵列矩阵理解有些模糊，但写完这个之后基本对矩阵的一些操作了然于心了，还有四元数也理解根深一些（尽管还没完全理解），还包括视图矩阵、投影矩阵的推导等等，这是写Math库的一些感悟。还有Light、Camera、Shader、Texture这些基础的就不说了，这里还想提的一个就是关于相机控制器，因为我工作内容有段时间就是在实现一套Google Earth的鼠标操作，所以还是花了点时间在相机控制方面，主要就是围绕相机姿态、位置还有插值这些知识的理解，还有定点旋转、第一人称、第三人称漫游等等，我可能会单独更新一篇博客关于如何写好相机控制器，现在这个版本也有很多没有移植出来。只是写了一套如何发送消息然后调用不同的Transform方法。还有一些零碎的模块由于还没测试或者还没写完所以就先不说了，后续会再更新。我直接贴一段目前这个版本的调用逻辑吧。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596Scene::ptr createScene()&#123; Scene::ptr scene = std::make_shared&lt;Scene&gt;(); //texture Texture::ptr earthTex = TextureManager::Inst()-&gt;loadTexture("earth", "../../../src/Data/texture/earthmap.jpg"); Texture::ptr boxTex = TextureManager::Inst()-&gt;loadTexture("cloud", "../../../src/Data/texture/box.jpg"); Texture::ptr floorTex = TextureManager::Inst()-&gt;loadTexture("earth", "../../../src/Data/texture/floor.jpg"); TextureUnitState::ptr earthUnit = std::make_shared&lt;TextureUnitState&gt;(); earthUnit-&gt;setTexture(earthTex); TextureUnitState::ptr boxUnit = std::make_shared&lt;TextureUnitState&gt;(); boxUnit-&gt;setTexture(boxTex); TextureUnitState::ptr floorUnit = std::make_shared&lt;TextureUnitState&gt;(); floorUnit-&gt;setTexture(floorTex); //material Material::ptr floorMat = std::make_shared&lt;Material&gt;(); floorMat-&gt;setMaterialType(Material::PhongMaterial); floorMat-&gt;setMap(floorUnit); floorMat-&gt;setCullFaceMode(CullFaceMode::DoubleSide); Material::ptr earthMat = std::make_shared&lt;Material&gt;(); earthMat-&gt;setMap(earthUnit); earthMat-&gt;setMaterialType(Material::PhongMaterial); Material::ptr boxMat = std::make_shared&lt;Material&gt;(); boxMat-&gt;setMaterialType(Material::PhongMaterial); boxMat-&gt;setMap(boxUnit); //mesh Mesh* floor = GeometryFactory::MakeQuad(100, 100); floor-&gt;setMaterial(floorMat); Mesh* box1 = GeometryFactory::MakeBox(20.0, 20.0, 20.0); box1-&gt;setPosition(Vector3D(-25.0, 10.0, 25.0)); box1-&gt;setMaterial(boxMat); Mesh* box2 = GeometryFactory::MakeBox(10.0, 10.0, 10.0); box2-&gt;setPosition(Vector3D(0.0, 5.0, -25.0)); box2-&gt;setMaterial(boxMat); Mesh* box3 = GeometryFactory::MakeBox(10.0, 10.0, 10.0); box3-&gt;setPosition(Vector3D(25.0, 5.0, 25.0)); box3-&gt;setMaterial(boxMat); Mesh* sphere = GeometryFactory::MakeSphere(8.0, 32, 32); sphere-&gt;setPosition(Vector3D(0.0, 8.0, 0.0)); sphere-&gt;setMaterial(earthMat); Object::ptr root = std::make_shared&lt;Object&gt;(); //light DirectionLight* dlight = new DirectionLight(); dlight-&gt;setPosition(Vector3D(0.0, 50.0, 50.0)); dlight-&gt;setShadowCamera(new OrthographicCamera(-50.0, 50.0, -50.0, 50.0, 0.1, 100.0)); PointLight* plight = new PointLight(); plight-&gt;setPosition(Vector3D(0.0, 50.0, 0.0)); plight-&gt;setShadowCamera(new PerspectiveCamera(MathHelper::radian(70.0), 1.0, 1.0, 200.0)); SpotLight* spotlight = new SpotLight; spotlight-&gt;setPosition(Vector3D(0.0, 30.0, 0.0)); spotlight-&gt;setAngle(M_PI/6.0); spotlight-&gt;setDecay(1.0); spotlight-&gt;setDistance(00.0); spotlight-&gt;setPenumbra(0.05); spotlight-&gt;setShadowCamera(new PerspectiveCamera(MathHelper::radian(50.0), 1.0, 1.0, 200.0)); root-&gt;add(floor); root-&gt;add(box1); root-&gt;add(box2); root-&gt;add(box3); root-&gt;add(sphere); root-&gt;add(dlight); root-&gt;add(plight); root-&gt;add(spotlight); scene-&gt;setSceneRoot(root); scene-&gt;setUseShadowMap(true); return scene;&#125;void main()&#123; Win::getSingleton()-&gt;create(); PerspectiveCamera::ptr camera = make_shared&lt;PerspectiveCamera&gt;(MathHelper::radian(65.0), (float)SCR_WIDTH / (float)SCR_HEIGHT, 0.1, 500.0); camera-&gt;setPosition(Vector3D(0.0f, 50.0f, -50.0)); camera-&gt;lookAt(0.0, 0.0, 0.0); Scene::ptr scene = createScene(); RenderSystem *rs = new RenderSystem(scene.get(), camera.get()); Win::getSingleton()-&gt;loadRenderSystem(rs); Win::getSingleton()-&gt;startRenderLoop(); delete rs;&#125; ​ ​ 这是一个简单的往Scene里面添加mesh和light，然后把scene和camera送到rendersystem开始渲染循环。 3.开源移植模块介绍​ 其实如果熟悉一些开源引擎，比如OSG、OGRE就立马看出很多东西都是从上面移植过来，比如完全移植了OGRE的HardwareBuffer，因为一开始看到这个就很酷，就是居然可以利用GPU来管理顶点数据，同时也可以在CPU上读写数据，我记得以前顶点数据我都是直接用vector容器管理，简直了。其实移植的过程就是理解或者是学习的过程，并不多单纯的复制粘贴，毕竟你没看到是不可能完全移植的，所以还是费了一点时间去理解它那套逻辑。这套东西可以管理顶点、索引和纹理数据，比HardwareVertexBuffer设计 1234567891011121314151617181920212223242526272829303132333435363738class HardwareVertexBuffer : public HardwareBuffer &#123; public: typedef std::shared_ptr&lt;HardwareVertexBuffer&gt; ptr; HardwareVertexBuffer(size_t vertex_size, size_t num_vertices, HardwareBuffer::Usage usage, bool use_shadow_buffer = false); ~HardwareVertexBuffer(); public: virtual void* lock(size_t offset, size_t length, LockOptions options); virtual void* lock(LockOptions options); virtual void unlock(void); virtual void readData(size_t offset, size_t length, void* dest); virtual void writeData(size_t offset, size_t length, const void* source, bool discardWholeBuffer = false); //virtual void copyData(HardwareBuffer&amp; src_buffer, size_t src_offset, size_t dst_offset, size_t length, bool discardWholeBuffer = false); //virtual void copyData(HardwareBuffer&amp; src_buffer); virtual size_t getSizeInBytes(void) const &#123; return _sizeInBytes; &#125; virtual Usage getUsage(void) const &#123; return _usage; &#125; virtual bool isLocked(void) const &#123; return _isLocked; &#125; virtual bool isUseShadowBuffer(void) const &#123; return _useShadowBuffer; &#125; virtual void upload(void); GLuint getBufferID() &#123; return _verexBufferID; &#125; unsigned int getVertexSize() &#123; return _vertexSize; &#125; unsigned int getVertexNum() &#123; return _numVertices; &#125; protected: GLuint _verexBufferID;//vbo size_t _numVertices;// size_t _vertexSize; size_t _sizeInBytes; Usage _usage; bool _isLocked; bool _useShadowBuffer; unsigned char* _data; &#125; ​ 它可以锁住一块内存，这块内存既可以是GPU也可以是CPU，然后进行读写数据，同时还可以设置shadowBuffer，是否在CPU端保存一份数据，比如在实际用的时候如下： 1234567891011121314151617181920212223242526272829303132333435363738394041Mesh* mesh = new Mesh;VertexData* vertexdata = new VertexData;vertexdata-&gt;setVertexStart(0);vertexdata-&gt;setVertexCount(4);VertexDeclaration::ptr vd = vertexdata-&gt;getVertexDeclaration();VertexBufferBinding::ptr bind = vertexdata-&gt;getVertexBufferBinding();size_t offset = 0;VertexElement::ptr tmp_ve = vd-&gt;addElement(0, offset, VET_FLOAT3, VES_POSITION);offset += tmp_ve-&gt;getTypeSize(VET_FLOAT3);tmp_ve = vd-&gt;addElement(0, offset, VET_FLOAT3, VES_NORMAL);offset += tmp_ve-&gt;getTypeSize(VET_FLOAT3);tmp_ve = vd-&gt;addElement(0, offset, VET_FLOAT2, VES_TEXTURE_COORDINATES);offset += tmp_ve-&gt;getTypeSize(VET_FLOAT2);char* data = (char*)malloc(sizeof(char)*vd-&gt;getVertexSize(0) * 4);float halfWidth = width / 2.0, halfHeight = height / 2.0;float vertices[32] = &#123; -halfWidth, 0.0, halfHeight, 0.0, 1.0, 0.0, 0.0, 0.0, halfWidth, 0.0, halfHeight, 0.0, 1.0, 0.0, 0.0, 1.0, -halfWidth, 0.0, -halfHeight, 0.0, 1.0, 0.0, 1.0, 0.0, halfWidth, 0.0, -halfHeight, 0.0, 1.0, 0.0, 1.0, 1.0&#125;;HardwareVertexBuffer* buffer = new HardwareVertexBuffer(offset, 4, HardwareBuffer::HBU_STATIC_WRITE_ONLY);bind-&gt;setBinding(0, (HardwareVertexBuffer::ptr)buffer);buffer-&gt;writeData(0, buffer-&gt;getSizeInBytes(), vertices);IndexData* indexdata = new IndexData;indexdata-&gt;setIndexStart(0);indexdata-&gt;setIndexCount(6);HardwareIndexBuffer * index_buffer = new HardwareIndexBuffer(HardwareIndexBuffer::IT_16BIT, 6, HardwareBuffer::HBU_STATIC_WRITE_ONLY);indexdata-&gt;setHardwareIndexBuffer((HardwareIndexBuffer::ptr)index_buffer);unsigned short faces[36] = &#123; 0,1,2, 2,1,3,&#125;;index_buffer-&gt;writeData(0, index_buffer-&gt;getSizeInBytes(), faces);mesh-&gt;setVertexData((VertexData::ptr)vertexdata);mesh-&gt;setIndexData((IndexData::ptr)indexdata);return mesh; ​ 可能这里与OGRE不同的地方是，我只是设置了一个简单的内存池去分割内存，OGRE的内存池管理不一样，具体看代码。 ​ 第二个借鉴的开源引擎就是工作中项目在用Threejs，它是WebGL版的开源库，当然它本身也有很多是借鉴OGRE的。我觉得Threejs之所以这么多人用就是它设计的非常好用，经常看到对图形学一点都不懂的在用Threejs，所以我就像我写的OpenGL版本能不能也这样设计，所以我借鉴了两个非常重要的设计，其一是它所有的对象都继承自Object3D，并且可以以节点的形式挂接，于是我设计了同样的类Object。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100class Mesh;class Light; class Sprite;class BillboardCollection;class ParticleSystem;class RayCaster;class TerrianTile;class Object &#123;public: typedef std::shared_ptr&lt;Object&gt; ptr; typedef std::vector&lt;Object::ptr&gt; Children; Object() :_position(Vector3D(0.0, 0.0, 0.0)), _scale(Vector3D(1.0, 1.0, 1.0)) &#123;&#125; Object(const Vector3D&amp; pos) :_position(pos), _scale(Vector3D(1.0, 1.0, 1.0)) &#123;&#125; Object(const Vector3D&amp; pos, const Quaternion&amp; quat) :_position(pos), _orientation(quat), _scale(Vector3D(1.0, 1.0, 1.0)) &#123;&#125;public: virtual Mesh* asMesh() &#123; return NULL; &#125; virtual const Mesh* asMesh() const &#123; return NULL; &#125; virtual Light* asLight() &#123; return NULL; &#125; virtual const Light* asLight() const &#123; return NULL; &#125; //virtual Plugin* asPlugin() &#123; return 0; &#125; //virtual const Plugin* asPlugin() const &#123; return 0; &#125; virtual Sprite* asSprite() &#123; return NULL; &#125; virtual const Sprite* asSprite() const &#123; return NULL; &#125; virtual BillboardCollection* asBillboardCollection() &#123; return NULL; &#125; virtual const BillboardCollection* asBillboardCollection() const &#123; return NULL; &#125; virtual ParticleSystem* asParticleSystem() &#123; return NULL; &#125; virtual const ParticleSystem* asParticleSystem() const &#123; return NULL; &#125; virtual TerrianTile* asTerrianTile() &#123; return NULL; &#125; virtual const TerrianTile* asTerrianTile() const &#123; return NULL; &#125; virtual Object* asObject() &#123; return this; &#125; virtual const Object* asObject() const &#123; return this; &#125; virtual void raycast(RayCaster* raycaster, Utils::AnyValue&amp; intersects) &#123; &#125;public: inline Object* getParent(); inline const Object* getParent() const &#123; return _parent; &#125; inline void setParent(Object* object) &#123; _parent = object; &#125; inline const Vector3D&amp; getPosition()const &#123; return _position; &#125; inline void setPosition(const Vector3D&amp; pos) &#123; _position = pos; &#125; inline const Vector3D&amp; getScale()const &#123; return _scale; &#125; inline void setScale(const Vector3D&amp; scale) &#123; _scale = scale; &#125; inline const Quaternion&amp; getOrientation()const &#123; return _orientation; &#125; inline void setOrientation(const Quaternion&amp; orientation) &#123; _orientation = orientation; &#125; void applyMatrix(const Matrix4D&amp; matrix); //_orientation set void setRotationFromAxisAngle(const Vector3D&amp; axis, double angle); void setRotationFromMatrix(const Matrix4D&amp; rotate); //_orientation change void rotateOnAxis(const Vector3D&amp; axis, double angle); void rotateOnAxisFixedPosition(const Vector3D&amp; axis, double angle);//special use void rotateOnX(double angle) &#123; rotateOnAxis(Vector3D(1.0, 0.0, 0.0), angle); &#125; void rotateOnY(double angle) &#123; rotateOnAxis(Vector3D(0.0, 1.0, 0.0), angle); &#125; void rotateOnZ(double angle) &#123; rotateOnAxis(Vector3D(0.0, 0.0, 1.0), angle); &#125; //for position change void translateOnAxis(const Vector3D&amp; axis, double distance); void translateOnX(double distance) &#123; translateOnAxis(Vector3D(1.0, 0.0, 0.0), distance); &#125; void translateOnY(double distance) &#123; translateOnAxis(Vector3D(0.0, 1.0, 0.0), distance); &#125; void translateOnZ(double distance) &#123; translateOnAxis(Vector3D(0.0, 0.0, 1.0), distance); &#125; Vector3D getDirection()const &#123; return _orientation * Vector3D(0, 0, 1); &#125; Vector3D getUp(void) const &#123; return _orientation * Vector3D(0, 1, 0); &#125; Vector3D getRight(void) const &#123; return _orientation * Vector3D(1, 0, 0); &#125; //useful function void localToWorld(Vector3D&amp; vector); void worldToLocal(Vector3D&amp; vector); // bool add(Object* object); bool remove(Object* object); Object::ptr getChild(int index); unsigned int getChildCount()const &#123; return _children.size(); &#125; //Children getChildren() &#123; return _children; &#125; void updateMatrixLocal(); void updateMatrixWorld(); // Matrix4D getLocalMatrix(); Matrix4D getWorldMatrix(); Vector3D getWorldPosition(); Quaternion getWorldQuaternion(); Vector3D getWorldScale();protected: Object* _parent; Vector3D _position; Vector3D _scale; Quaternion _orientation; Children _children; Matrix4D _matrix_local; Matrix4D _matrix_world;&#125;; ​ 所有的Actor（UE4里面叫做Actor），我觉得非常合适，都有这些通用的属性信息，并且同时有OSG节点的设计，可以很好的连接场景，渲染的时候从根节点开始更新数据（主要是更新矩阵信息），然后不同的类型对场景又有不同的作用。Scene只需要记录根节点信息即可，具体见代码。 ​ 其二移植的地方就是Material，Threejs的不同的Material可以组装成不同的shader，从而得到不同的效果，目前移植了BasicMaterial和PhongMaterial，一个没有光照，一个有光照。通过Material设置属性，然后再渲染的时候根据材质类型获取需要的属性信息给uniform赋值，感觉这样的设计很不错，同时也移植了Threejs的BRDF的光照模型，还没测试GGX的那套，只测试了简单的，感觉效果可以，关于物理光照，后期会整理一篇文章，因为最近也在看这个。对于Material的移植可能后期还会详细讲述，因为目前还没完全移植，比如PhongMaterial加了光照效果如下。 更多实际效果以及具体代码看Github。 ​ 这两块是大块的移植，当然还有其它一些小的地方，就不一而足了。其实本来想把OGRE的ResourceManager这套移植过来，后来好像没啃下来，主要是感觉对我目前的这个版本实用性不大，至多就是有个TextureManager或者ShaderManager，如果后期会有多个场景的概念，再移植它这种资源的加载卸载机制来管理不同场景的资源。 4.最后​ 说实话写的时候一直在问自己一个问题，这有意义吗？别人开源引擎都封装的很好，用好它就可以做出很炫的效果，而且时间成本很低。的确如此，这可能并不是最有效率的学习方式，但却是我最喜欢的学习方式。我希望自己能有一件自己感兴趣的事情，一直去坚持写，坚持研究下去，仅仅是因为喜欢，不去追求它的实际意义。而些SimpleRenderEngine可能就是这么一件事情，我喜欢写代码，喜欢做一些酷炫的图形效果，然后就有了这件事，这可能就是它的意义吧，就是因为喜欢。 ​ 最后希望志同道合之人加入QQ群（528379336）一起讨论，同时欢迎fork Github地址：https://github.com/LukiYLS/SimpleRenderEngine]]></content>
      <categories>
        <category>SimpleRenderEngine</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>OpenGL</tag>
        <tag>OSG</tag>
        <tag>OGRE</tag>
        <tag>ThreeJS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始写渲染引擎-开篇]]></title>
    <url>%2F2017%2F09%2F05%2F%EF%BC%88url%E4%B8%AD%E6%98%BE%E7%A4%BA%E7%9A%84%E6%A0%87%E9%A2%98%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言​ 最近 准备开始写一个自己的渲染引擎，主要有两个目的：一是为了系统的学习关于如何从底层开始写一个渲染引擎，二是为了以后方便给自己用；我会在博客和github上同步更新我的进度，博客的目的是为了把自己开发过程中的思路记录下来，同时也希望能找到志同道合之人一起学习进步，如果自己的经验能够给别人有点帮助那就最好不过了，同时所有的代码都会更新到github当中，如果有人能加入一起造轮子，那我会非常欢迎。 ​ 其实写之前也犹豫了很久，很多人可能会问，现在这么多开源的引擎，为什么不直接用呢，原因其实很简单，就像我之前讲过我对开源的理解，开源并不是说我们能用它做多少事情，而是我们能从开源当前学会多少东西。现在越来越少的人会去自己造轮子，都是直接用现成的东西，所以经常会遇到很多半桶水水平的人，这种人真多很烦，因为我是GIS专业的，而且也是在GIS公司，公司有些所谓的 老鸟，用过一些开源的引擎做了点东西，以为自己对图形学很熟悉了，可是经常连模板测试都一知半解的，然后还跟你争执，真是无语了，这让我更加意识到自己需要把基础打好，不能步其后尘。而造轮子是非常好的学习方法，很多东西真的需要自己动手去一个坑一个坑的跳，你才能对一些知识理解的更深入一点。就比方说学习编译原理最好的方法就是自己写个编译器。 ​ 我可能会先从OpenGL开始写起，后面会考虑慢慢更新WebGL版本，不过WebGL版本可能会倾向于数据可视化方面，因为这也是我想研究的一个方向，现在可能自己还需要学习一下，毕竟现在Web端也算是一个趋势了，不过两个版本如果用同一个架构的话，移植还是挺好的，所以，目前还是先把OpenGL版本写好吧！ SimpleRenderEngine V1.0框架设计​ 其实我写这篇博客的时候已经完成了一些基础的框架设计，主要是实现了一些基础类，比如Texture，Light，Camera，Entity，Scene，Mesh我等等。要说框架设计，好像没什么东西，就是先想一个简单的方法把它们串到一起，后面可能要慢慢重构吧，因为第一版本没考虑很高深的架构设计，但我写之前对自己引擎框架的要求就是，外部尽量简单，所以就需要里面能很好的连在一起，因为现在模块也比较少，所以把它们联系到一起很简单。先看下我这个引擎绘制一个cube外部代码大概向下面这样： 12345678910111213141516171819202122232425262728293031Win::Inst()-&gt;createWindow(); vector&lt;Vertex&gt; vertices;vertices.push_back(Vertex(0.5f, 0.5f, 0.0f, 0, 0, 1, 1, 1));vertices.push_back(Vertex(0.5f, -0.5f, 0.0f, 0, 0, 1, 1, 0));vertices.push_back(Vertex(-0.5f, -0.5f, 0.0f, 0, 0, 1, 0, 0));vertices.push_back(Vertex(-0.5f, 0.5f, 0.0f, 0, 0, 1, 0, 1));vector&lt;unsigned int&gt; indices = &#123; 0, 3, 1, 1, 3, 2 &#125;;Mesh::ptr mesh = std::make_shared&lt;Mesh&gt;();mesh-&gt;setVertices(vertices);mesh-&gt;setIndex(indices); mesh-&gt;createBuffer(); Light::ptr light = std::make_shared&lt;Light&gt;();light-&gt;setType(PointLight);TextureManager::Inst()-&gt;loadTexture("../../../src/Data/texture/1.jpg", "texture1");TextureManager::Inst()-&gt;loadTexture("../../../src/Data/texture/2.jpg", "texture2");TextureManager::Inst()-&gt;loadTexture("../../../src/Data/texture/3.jpg", "texture3");Camera::ptr camera = std::make_shared&lt;Camera&gt;(glm::vec3(0.0f, 0.0f, 3.0f));camera-&gt;setPerspectiveFovLHMatrix(glm::radians(45.0f), (float)SCR_WIDTH / (float)SCR_HEIGHT, 0.1f, 100.0f);Shader::ptr shader = std::make_shared&lt;Shader&gt;("../../../src/Data/shader/basic.vs", "../../../src/Data/shader/basic.fs"); mesh-&gt;setProgram(shader); mesh-&gt;addTexture("texture3");Scene::Inst()-&gt;addEntity("test", (Entity::ptr)mesh);Scene::Inst()-&gt;addLight(light);Win::Inst()-&gt;starup(camera); ​ 其实这个很简单，就是数据丢到mesh当中，纹理丢到textureManager中，然后建个camera，把mesh加到scene中，绘制，OK。具体内部怎么调用，大家可以到github把代码git下来看一下。 ​ 我可能不会在博客里将太多理论的东西，因为红宝书上都有，如果有个别不能理解的可以提问，可能后续会有些原理复杂一点的我会专门写篇博客去讲解，我会把思路，其实应该是框架性的东西，分享出来，当我写的过程中不知怎么办的时候，我也会写出来跟大家交流。我是个小白，所以也会有很多问题，希望能得到大牛的指点，也希望自己能坚持下去，只是因为喜欢。]]></content>
      <categories>
        <category>从零开始写渲染引擎</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>OpenGL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Earth开发原理-基于LRU-K实现瓦片数据析构算法]]></title>
    <url>%2F2017%2F09%2F02%2F%EF%BC%88url%E4%B8%AD%E6%98%BE%E7%A4%BA%E7%9A%84%E6%A0%87%E9%A2%98%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 用户访问三维earth过程中，客户端会源源不断的向服务器请求数据，尤其是多用户并发的情况下，请求更加频繁，所以服务器通常会设置缓存容器，主要是DEM和DOM缓存池，数据从server中请求后，会在服务器同时保留一份。比如请求google 影像，服务器根据wtms服务地址以及用户请求的瓦片层级，然后利用HTTP协议将数据传输至服务器，这时候服务器就将数据进行备份，这样等下一次在请求同样的数据时，就可以直接从服务器中获取，这样可以提高用户访问效率。然而，随着用户访问的增加，服务器缓存压力必然也会加大，所以就需要及时析构一些数据，保证服务器缓存池不会爆。 LRU和LRU-K算法概述​ 缓存淘汰算法包括LRU、LFU和FIFO，关于这三种算法的比较，可以看一这篇博客： 缓存算法（页面置换算法）-FIFO、LFU、LRU ​ 这里只对LRU-K（LRU-K改进）算法作一个简单介绍，LRU或者LRU-K缓存析构算法是一种比较简单常用的缓存淘汰机制，LRU主要根据资源访问时间来淘汰数据，也就是说访问数据间隔越久，其淘汰顺序应该是月优先，所以在设计算法时，只需要设计一个队列来存储每个资源的访问时间，新访问的数据放到最前，这样就可以保证末尾的数据时间间隔最久，每次淘汰只需从末尾开始淘汰。 ​ 而LRU-K，则是增加了一个访问次数变量，通过访问次数K来控制需要淘汰的数据，所以它需要多维护一个队列来记录访问次数， 数据第一次被访问，加入到访问历史列表； 如果数据在访问历史列表里后没有达到K次访问，则按照一定规则（FIFO，LRU）淘汰； 当访问历史队列中的数据访问次数达到K次后，将数据索引从历史队列删除，将数据移到缓存队列中，并缓存此数据，缓存队列重新按照时间排序； 缓存数据队列中被再次访问后，重新排序； 需要淘汰数据时，淘汰缓存队列中排在末尾的数据，即：淘汰“倒数第K次访问离现在最久”的数据。 瓦片数据析构算法实现​ LRU-K淘汰机制可以很好的应用到瓦片数据的析构中，因为访问次数和时间两个权重可以通时考虑到用户访问的效率以及缓存池的压力，所以考虑将此方法应用到DEM和DOM缓冲析构当中，首先要考虑的是两个参数的确定，时间当然是指用户的访问某瓦片的时间，次数表示某瓦片被用户访问的次数，每个用户只会记录一次，这里需要对LRU-K作一个小小的改进，因为我们考虑析构的原则是：很久没被访问，且访问次数少的瓦片应该优先淘汰，所以需要把这两个权重都加入到用户访问记录中去。 ​ 综合考虑，析构算法主要思路是：当用户请求瓦片时，记录该瓦片的访问时间，并且增加瓦片访问次数（+1操作）。每次请求数据时，通过时间来更新访问次数，如果访问时间间隔大于给定的最大时间，则访问次数（-1操作）。通过+1和-1两个操作控制瓦片访问次数，当内存池容量超过一定阈值，需要执行析构时，从访问记录中取前K个访问次数较少的瓦片，释放其资源。 ​ LRU Node的设计如下： 1234567891011121314151617class lru_node &#123; friend class lru_queue; public: lru_node(); virtual ~lru_node(); public: void Hit(unsigned int time_now_); unsigned int get_time(); void bind_queue(lru_queue* queue_); void unbind_queue(); protected: unsigned int _last_use_time; lru_queue* _lru_queue; std::list&lt;lru_node*&gt;::iterator _self; &#125;; ​ 可以看出，这里只是设计了节点的访问时间，并且维护了一个队列，lru_queu的设计如下： 12345678910111213141516171819202122232425262728class lru_queue &#123; friend class lru_node; public: typedef gw_shared_ptr&lt;lru_queue&gt; ptr; public: lru_queue(runtime* run_time_, terrian_dem_resource_manager* dem_manager_); lru_queue(runtime* run_time_, terrian_dom_resource_manager* dom_manager_); ~lru_queue(); public: void move_top(lru_node* node_); void remove(lru_node* node_); unsigned int get_time(); lru_node* pop(); int get_queue_size(); void update(unsigned int lost_time_); protected: std::list&lt;lru_node* &gt; _list_queue; private: gw::mutex::ptr _queue_mutex; //union //&#123; enum Type&#123;DEM,DOM&#125;_type; terrian_dem_resource_manager* _dem_manager; terrian_dom_resource_manager* _dom_manager; //&#125;; &#125;; ​ 其中update函数就是通过时间来更新瓦片访问次数，就是从尾部遍历队列，如果访问时间与当前时间间隔大于lost_time，则只需-1操作。 ​ +1操作如下： 123456789101112131415161718192021222324252627void add_user_visit_record(std::string userid, geocode gc) &#123; lru_tile* node = new lru_tile(); node-&gt;_gc = gc; //dom_rm-&gt;_dom_list-&gt;move_top(node); unsigned int timt_now = get_tick_count(); node-&gt;Hit(timt_now); _queue_service-&gt;move_top(node, userid); _visit_record_mutex-&gt;lock(); std::vector&lt;RECORD_TYPE&gt;::iterator it = _visit_record_vec.begin(); for (; it != _visit_record_vec.end(); ++it) &#123; if (it-&gt;first == gc) &#123; it-&gt;second++; break; &#125; &#125; if (it == _visit_record_vec.end()) &#123; RECORD_TYPE record = std::make_pair(gc,1); _visit_record_vec.push_back(record); &#125; _visit_record_mutex-&gt;unlock(); &#125; 与之对应的-1操作如下： 1234567891011121314void visit_count_plus_one(geocode gc_)&#123; _visit_record_mutex-&gt;lock(); std::vector&lt;RECORD_TYPE&gt;::iterator it = _visit_record_vec.begin(); for (; it != _visit_record_vec.end(); ++it) &#123; if (it-&gt;first == gc_) &#123; it-&gt;second--; break; &#125; &#125; _visit_record_mutex-&gt;unlock();&#125; 总结​ 这个析构算法，虽然到现在并没有真正测试多用户并发情况下的效率，只是可以保证内存池能够稳定在阈值之内，可以到实际情况，还需要不断的优化和改进，因为数据加载效率也是非常重要的考量。]]></content>
      <categories>
        <category>Earth开发原理</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Earth开发原理-阴影体实现要素贴地]]></title>
    <url>%2F2017%2F08%2F26%2F%EF%BC%88url%E4%B8%AD%E6%98%BE%E7%A4%BA%E7%9A%84%E6%A0%87%E9%A2%98%EF%BC%89%2F</url>
    <content type="text"><![CDATA[关于阴影实现方法概述​ Shadow map: 阴影的实现技术，我以前实现过比较基础的shadow map，就是利用从光源出发得到的深度图，然后每一个像素去比较当前深度值与深度图中的得到的深度值，深度图其实保存的应该是光源到遮光板之间的距离，而与它比较的是像素到光源的距离，如果小于，则说明处于阴影之中。还有一些细节的改进，比如阴影交叉如何处理（与深度图的分辨率有关，如果多个像素用同一个比较，就会出现这种情况），还有锯齿如何处理等等。 具体可以看这篇教程：Shadow map。 ​ Shadow volume: 上面这种方法会有两个问题：1.如果光源动态变化，则每一次shadow map都要更新，这样开销会很大；2.上述出现的交叉阴影和锯齿问题，都没有一种很好的方法完美解决。因此Franklin C. Crow在1977年 提出了一种新的方法：Shadow volume。不过这种方法好像对于多光源的情况下，也并无优势。所以现在主流的依然是shadow map方法，可以结合deferred lighting技术。不过因为今天的主题是要用到阴影体这种方法，所以下面线详细介绍一下这种方法。 Shadow volume实现方法​ shadow volume主要用到的原理是模板测试和深度测试，通过这两个测试，把阴影部分的stencil buffer计算出来，然后render这部分就可得到阴影。 ​ 第一步：构建阴影体。对mesh每一条边沿着光源方向进行拉伸，顶点部分除了原始的n个顶点(x,y,z,1)外，拉伸后的顶点同样也有n个，拉伸的长度应是无限远，但我们只有将其顶点形式设置为（x,y,z,0），在shader里面转换后自然就是无穷远处。面主要包括顶面、底面和侧面，底面三角形索引结构其实和顶面是一样的，侧面就是每一个quad由两个三角形组成，这样就可以得到阴影体的vertex buffer。 ​ 第二步：Render pass。 ​ pass1：打开depth test，按正常方式渲染整个场景，得到depth map。 ​ pass2：打开stencil test，关掉z writing和color buffer writing，渲染shadow volumes；设置stencil test always pass，对于front faces，若z test pass，则stencil value +1，若z test fail，则不更新stencil value；对于back faces，若z test pass，则stencil value -1。 ​ pass3：pass2完成之后，stencil buffer中value不为0的像素就处于阴影区域，据此绘制阴影效果即可。 12345678910111213141516//pass1RenderVolumetoDepth();//pass 2glDepthMask(GL_FALSE);glEnable(GL_DEPTH_CLAMP);glDisable(GL_CULL_FACE);glStencilFunc(GL_ALWAYS, 0, 0xff); glStencilOpSeparate(GL_BACK, GL_KEEP, GL_INCR_WRAP, GL_KEEP);glStencilOpSeparate(GL_FRONT, GL_KEEP, GL_DECR_WRAP, GL_KEEP);RenderVolume();//pass 3glStencilFunc(GL_EQUAL, 0x0, 0xFF); glStencilOpSeparate(GL_BACK, GL_KEEP, GL_KEEP, GL_KEEP);RenderVolume();glDisable(GL_STENCIL_TEST);...... ​ 这样就可以实现一个简单的阴影效果了，其中核心就是利用模板测试标记出阴影部分，当然还有一些细节，比如多光源的时候需要遍历每一个光源，想根深一步了解可以看《GPU Gems》Chapter 9. Efficient Shadow Volume Rendering. 利用阴影体实现贴地效果​ 前面简单的介绍了一下shadow volume的原理，下面要进入正题，要素贴地。这算是三维球中一个很基本的功能，因为这会涉及要标会，量测，矢量要素加载等等，所以这个问题也必须解决，才能有后续这些相关功能的实现。一开始对这个没什么概念，想了很多方法，第一反应到的就是纹理。在有DEM数据的情况下，面对高低起伏的地形，如何将要素随着地形起伏，感觉只有将其作为一张纹理贴地和瓦片数据一样贴到地形网格上，才能完美贴地，好像osgearth就是这样做的，所以就去研究投影纹理技术，投影纹理就不展开讲了，就类似虚拟一个投影仪，然后将纹理投影到物体表面。 ​ 阴影体这个方法是看了cesium的源码才知道原来贴地还可以这样，关于cesium后面应该还会有很多篇博客来讲其中的一些图形学知识，因为近期一直在研究它的源码，cesium算是web端的标杆了，类似pc端的oe，现在很多都在用它搞二次开发，比如超图。我们虽然没有直接用它，但是很多地方都在借鉴它的一些原理，其实我觉得这才是开源的意义，开源并不是让你可以直接用它进行二次开发，更主要的是你能够弄懂里面的一些原理，并为自己所用，这叫站在巨人的肩膀上。现在很多所谓的三维GIS创业公司，不过是做了点二次开发，包装了一个界面，就拿出来骗钱，非常可笑。不过也是，这几年政府的钱太好骗了，不过这样的没点技术积累的公司基本也存活不了多久。好了，不说废话了，开始装逼了。 ​ 前面说了阴影体怎么构建，那到了有DEM的情况下，又应该如何构建呢？比如说画一个Polygon，首先要做的就是拉伸，那拉伸又包括拉伸方向和拉伸高度，拉伸方向很简单就是顶点法线，拉伸高度应该与polygon所在地形高程与关，必须保持拉伸高度超过其范围之内的最大高程，所以拉伸高度就是高程的最大值。 ​ 阴影体构建之后，又如何只绘制贴地部分呢，这就要用到前面说的模板测试和深度测试了，这里有会有三个pass，不过这三个pass与前面的有点区别。 1234567891011121314151617181920212223242526272829//pass 1gl.colorMask(false,false,false,false);gl.depthMask(false);gl.enable(gl.STENCIL_TEST);gl.disable(gl.DEPTH_TEST);gl.disable(gl.CULL_FACE);gl.stencilFuncSeparate(gl.FRONT,gl.ALWAYS,0,~0);gl.stencilFuncSeparate(gl.BACK,gl.ALWAYS,0,~0);gl.stencilOpSeparate(gl.BACK,gl.KEEP,gl.DECR,gl.INCR_WRAP); gl.stencilOpSeparate(gl.FRONT,gl.KEEP,gl.DECR,gl.DECR_WRAP);renderShadowVolume(camera);//pass 2gl.enable(gl.DEPTH_TEST);gl.depthFunc(gl.LEQUAL);gl.stencilFuncSeparate(gl.FRONT,gl.ALWAYS,0,~0);gl.stencilFuncSeparate(gl.BACK,gl.ALWAYS,0,~0); gl.stencilOpSeparate(gl.BACK,gl.KEEP,gl.KEEP,gl.INCR_WRAP);gl.stencilOpSeparate(gl.FRONT,gl.KEEP,gl.KEEP,gl.DECR_WRAP);renderShadowVolume(camera);//pass 3gl.colorMask(true,true,true,true);gl.enable(gl.STENCIL_TEST);gl.enable(gl.BLEND);gl.disable(gl.DEPTH_TEST);gl.stencilFunc(gl.NOTEQUAL,0,~0);gl.stencilOp(gl.KEEP,gl.KEEP,gl.DECR);renderShadowVolume(camera);//set backgl.depthMask(true);gl.disable(gl.STENCIL_TEST); gl.enable(gl.CULL_FACE); gl.enable(gl.DEPTH_TEST); ​ pass 1：关闭颜色写入以及背面裁剪（因为是双面绘制，模板函数不一样），开启模板测试，关闭深度测试，正面模板值-1，背面模板值+1 ​ pass 2：开启深度测试和模板测试，同样正面-1，背面加+1 ​ pass 3：开启颜色写入，将模板值不为0的区域绘制出来，自然就是和地形相交的部分，就可以看到贴地效果。 ​ 这里有个很有意思的地方，就是只要pass 2和pass 3其实就可以标记出贴地部分，但如果你camera进入到阴影体区域，应该通过关闭深度测试来标记出把地形遮挡的区域，所以pass 1的目的其实是为了标记出进入阴影体内部的贴地区域，这两个标记需要分成两个pass，这样即使你进入阴影体内部也可以达到同样的效果。 总结​ 这算是我我开通博客站点后的第一篇博客，我就喜欢这种直接上干货的形式，毕竟这也是我想写博客的目的，积累和分享。同时也希望能保存博客的质量，只要是拿出来的，一定要有价值。这篇博客的分类是Earth开发原理，意味着以后我会陆续把我在开发三维earth过程中，一些有价值，有意思的技术分享出来，但我并不会直接把源码放出来，因为我分享的技术。也希望有看到的朋友多提建议，共同交流。 ​ 预告：下一篇可能会是google earth相机实现]]></content>
      <categories>
        <category>Earth开发原理</category>
      </categories>
      <tags>
        <tag>三维GIS</tag>
        <tag>图形算法</tag>
        <tag>WebGL</tag>
      </tags>
  </entry>
</search>
